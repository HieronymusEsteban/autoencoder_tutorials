{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smjlo1x48N3t"
   },
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6H2W9QmdP0U"
   },
   "source": [
    "<p>\n",
    "CAS on Advanced Machine Learning <br>\n",
    "Data Science Lab, University of Bern, 2024<br>\n",
    "Prepared by Dr. Mykhailo Vladymyrov.\n",
    "\n",
    "</p>\n",
    "\n",
    "This work is licensed under a <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZiWkvrVdiiA"
   },
   "source": [
    "# Libs and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GynY3qcttQPu"
   },
   "outputs": [],
   "source": [
    "# on colab:\n",
    "# !pip install einops\n",
    "# !pip install mlflow\n",
    "# !pip install optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TDxRnvL7x3Q"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjo8ffOptAcP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set env var to allow duplicated lib\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "#This code snippet is doing the following:\n",
    "\n",
    "#import os - Imports Python's built-in operating system module, which provides functions for interacting with the operating system.\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='TRUE' - Sets an environment variable called 'KMP_DUPLICATE_LIB_OK' to 'TRUE'.\n",
    "\n",
    "#This specific environment variable is related to Intel's Math Kernel Library (MKL), which PyTorch often uses for performance optimization on Intel CPUs. The setting is addressing a known issue where the Intel OpenMP runtime library might be loaded multiple times, which can cause warnings or errors on some systems (particularly macOS).\n",
    "#By setting this to 'TRUE', you're essentially telling the system to ignore the duplicate library loading issue. This is a common workaround when using PyTorch on macOS to prevent warnings about duplicate libraries being loaded.\n",
    "#This line doesn't affect the functionality of your model, but rather helps avoid environment-related warnings or errors during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRCK8pzztAcQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import einops as eo\n",
    "import pathlib as pl\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import collections  as mc\n",
    "from matplotlib import animation\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from time import time as timer\n",
    "#import umap\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Audio\n",
    "import IPython\n",
    "\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import sys\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "\n",
    "from source import image_id_converter as img_idc\n",
    "from source import load_process_images as lpi\n",
    "\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlSvdkTItAcR"
   },
   "outputs": [],
   "source": [
    "# get mean and std of an array with numpy:\n",
    "def get_mean_std(x):\n",
    "    x_mean = np.mean(x)\n",
    "    x_std = np.std(x)\n",
    "    return x_mean, x_std\n",
    "\n",
    "# get min and max of an array with numpy:\n",
    "def get_min_max(x):\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    return x_min, x_max\n",
    "\n",
    "def is_iterable(obj):\n",
    "    try:\n",
    "        iter(obj)\n",
    "    except Exception:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "#This function checks if an object is iterable (can be looped over).\n",
    "#It uses a try-except block to attempt to call iter(obj), which will succeed only if obj is iterable.\n",
    "#If calling iter(obj) raises any exception, the function returns False.\n",
    "#If no exception occurs, the function returns True.\n",
    "\n",
    "def type_len(obj):\n",
    "    t = type(obj)\n",
    "    if is_iterable(obj):\n",
    "        sfx = f', shape: {obj.shape}' if t == np.ndarray else ''\n",
    "        print(f'type: {t}, len: {len(obj)}{sfx}')\n",
    "    else:\n",
    "        print(f'type: {t}, len: {len(obj)}')\n",
    "\n",
    "#This is a utility function for debugging that prints information about an object.\n",
    "#t = type(obj) - Gets the type of the provided object.\n",
    "#It checks if the object is iterable using the is_iterable function defined earlier.\n",
    "#If the object is iterable:\n",
    "#\n",
    "#It checks if the object is a NumPy array (t == np.ndarray).\n",
    "#If it's a NumPy array, it adds shape information to the output string.\n",
    "#It prints the type and length of the object, along with shape information if applicable.\n",
    "#\n",
    "#\n",
    "#If the object is not iterable, it still attempts to print the type and length (though this might raise an error if len() isn't applicable to the object).\n",
    "#\n",
    "#Note: There seems to be an issue with the type_len function - it tries to call len() on non-iterable objects in the else clause, \n",
    "#which would typically cause an error. This might be a bug in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N78bzRywtAcc"
   },
   "outputs": [],
   "source": [
    "def to_np_showable(pt_img):\n",
    "  np_im = pt_img.detach().cpu().numpy()\n",
    "  if len(np_im.shape) == 4:\n",
    "    np_im = np_im[0]\n",
    "\n",
    "  if np_im.shape[0] > 3:\n",
    "    np_im = np_im[-3:]\n",
    "\n",
    "  return (eo.rearrange(np_im, 'c h w -> h w c')/2+.5).clip(0., 1.)\n",
    "\n",
    "#This function converts a PyTorch tensor image to a NumPy array suitable for visualization.\n",
    "#pt_img.detach().cpu().numpy() - Detaches the tensor from the computation graph, moves it to CPU if it's on GPU, and converts it to a NumPy array.\n",
    "#if len(np_im.shape) == 4: - Checks if the image has a batch dimension (shape: [batch, channels, height, width]).\n",
    "#np_im = np_im[0] - If there's a batch dimension, takes only the first image in the batch.\n",
    "#if np_im.shape[0] > 3: - Checks if there are more than 3 channels.\n",
    "#np_im = np_im[-3:] - If there are more than 3 channels, keeps only the last 3 channels (useful for handling multi-channel data).\n",
    "#eo.rearrange(np_im, 'c h w -> h w c') - Uses the einops library to rearrange the tensor from PyTorch's [channels, height, width] format to matplotlib's [height, width, channels] format.\n",
    "#/2+.5 - Applies normalization assuming the image data is in the range [-1, 1], converting it to [0, 1].\n",
    "#.clip(0., 1.) - Ensures all values are within the [0, 1] range, clamping any values outside this range.\n",
    "\n",
    "def plot_im(im, is_torch=True):\n",
    "  plt.imshow(to_np_showable(im) if is_torch else im, cmap='gray')\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "#This function plots a single image.\n",
    "#is_torch=True - Default parameter indicating whether the input is a PyTorch tensor.\n",
    "#to_np_showable(im) if is_torch else im - Converts the image to a NumPy array if it's a PyTorch tensor, otherwise uses it directly.\n",
    "#plt.imshow(..., cmap='gray') - Displays the image using matplotlib with a grayscale colormap.\n",
    "#plt.show() - Renders the plot.\n",
    "#plt.close() - Closes the figure to free up memory.\n",
    "\n",
    "def plot_im_samples(ds, n=5, is_torch=False):\n",
    "  fig, axs = plt.subplots(1, n, figsize=(16, n))\n",
    "  for i, image in enumerate(ds[:n]):\n",
    "      axs[i].imshow(to_np_showable(image) if is_torch else image, cmap='gray')\n",
    "      axs[i].set_axis_off()\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "#This function plots multiple images from a dataset in a row.\n",
    "#ds - The dataset or collection of images to sample from.\n",
    "#n=5 - Default number of images to display.\n",
    "#is_torch=False - Default parameter indicating whether the inputs are PyTorch tensors.\n",
    "#plt.subplots(1, n, figsize=(16, n)) - Creates a figure with a single row of n subplots, with a width of 16 inches and height of n inches.\n",
    "#The loop iterates through the first n images in the dataset:\n",
    "#\n",
    "#axs[i].imshow(...) - Displays each image in its corresponding subplot.\n",
    "#axs[i].set_axis_off() - Removes axis labels and ticks for cleaner visualization.\n",
    "#\n",
    "#\n",
    "#plt.show() - Renders the entire plot with all images.\n",
    "#plt.close() - Closes the figure to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyC2UKsetAce"
   },
   "outputs": [],
   "source": [
    "# merging 2d matrix of images in 1 image\n",
    "def mosaic(mtr_of_ims):\n",
    "  ny = len(mtr_of_ims)\n",
    "  assert(ny != 0)\n",
    "  #Gets the number of rows in the matrix and asserts that it's not empty.\n",
    "\n",
    "  nx = len(mtr_of_ims[0])\n",
    "  assert(nx != 0)\n",
    "  #Gets the number of columns in the first row and asserts that it's not empty.\n",
    "\n",
    "  im_sh = mtr_of_ims[0][0].shape\n",
    "\n",
    "  assert (2 <= len(im_sh) <= 3)\n",
    "  #Gets the shape of the first image in the matrix.\n",
    "  #Verifies that the image is either 2D (grayscale) or 3D (with channels).\n",
    "    \n",
    "  multichannel = len(im_sh) == 3\n",
    "\n",
    "  if multichannel:\n",
    "    h, w, c = im_sh\n",
    "  else:\n",
    "    h, w = im_sh\n",
    "  #Determines if the images have multiple channels.\n",
    "  #If multichannel, unpacks height, width, and channels. Otherwise, just height and width.\n",
    "\n",
    "  h_c = h * ny + 1 * (ny-1)\n",
    "  w_c = w * nx + 1 * (nx-1)\n",
    "  #Calculates the total height and width of the canvas.\n",
    "  #Adds 1 pixel spacing between images (both horizontally and vertically).\n",
    "\n",
    "  canv_sh = (h_c, w_c, c) if multichannel else (h_c, w_c)\n",
    "  canvas = np.ones(shape=canv_sh, dtype=np.float32)*0.5\n",
    "  #Defines the shape of the canvas based on whether images are multichannel.\n",
    "  #Creates a canvas filled with gray (0.5) values, assuming image values are in [0,1] range.\n",
    "\n",
    "  for iy, row in enumerate(mtr_of_ims):\n",
    "    y_ofs = iy * (h + 1)\n",
    "    #Loops through each row of images.\n",
    "    #Calculates the vertical offset for the current row.\n",
    "    for ix, im in enumerate(row):\n",
    "      x_ofs = ix * (w + 1)\n",
    "      #Loops through each image in the current row.\n",
    "      #Calculates the horizontal offset for the current image.\n",
    "      canvas[y_ofs:y_ofs + h, x_ofs:x_ofs + w] = im\n",
    "      #Copies the current image to the appropriate position in the canvas.\n",
    "      #This uses NumPy's array slicing to place the image at the correct location.\n",
    "  return canvas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "FHjhiIzRtAcf"
   },
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')  # use first available GPU\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se 'mps' Ã¨ disponibile su Apple Silicon\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "\n",
    " \n",
    "\n",
    "# Conferma del dispositivo selezionato\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error message when MPS is out of memory: \n",
    "MPS backend out of memory (MPS allocated: 5.65 GB, other allocations: 492.00 KB, max allowed: 5.10 GB). Tried to allocate 500.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution proposed on: https://discuss.pytorch.org/t/mps-backend-out-of-memory-in-mac/198245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "firstly run this command in notebook cell\n",
    "\n",
    "torch.mps.set_per_process_memory_fraction(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to confirm that this solution does not damage anything first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5NaXhkctAcg",
    "outputId": "55e8045c-60a2-4dec-d19f-37d7f0fb9619"
   },
   "source": [
    "device = get_device()\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux0WVvO6diiD"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3xcUuod8bUt"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMWqy2uMffPA"
   },
   "source": [
    "Lets start with a simple, well understood mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSU2wVsXtAcj"
   },
   "outputs": [],
   "source": [
    "NOISE_RATE = 0.1\n",
    "N_SAMPLE = 32\n",
    "N_VIS_SAMPLE = 2\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSPzUDchtAck"
   },
   "outputs": [],
   "source": [
    "def collate_ae_dataset(samples):\n",
    "    \"\"\"\n",
    "    The function collates sampels into a batch, and creates noisy samples if DENOISING is True\n",
    "    for the denoising autoencoder.\n",
    "    \"\"\"\n",
    "    xs = [s[0] for s in samples]\n",
    "    ys = [s[1] for s in samples]\n",
    "    #Extracts the first element (input data) from each sample into list xs.\n",
    "    #Extracts the second element (labels or targets) from each sample into list ys.\n",
    "    #This assumes each sample is a tuple or list with at least two elements.\n",
    "    \n",
    "    xs = torch.stack(xs)\n",
    "    ys = torch.concat(ys)\n",
    "    #torch.stack(xs) combines the list of input tensors into a single \n",
    "    #tensor along a new dimension (creating a batch dimension).\n",
    "    #torch.concat(ys) concatenates the label tensors along \n",
    "    #the existing first dimension. This suggests the labels might have \n",
    "    #variable lengths or already include a batch-like dimension.\n",
    "\n",
    "    add_noise = NOISE_RATE > 0.\n",
    "    #Checks if noise should be added based on a global variable NOISE_RATE.\n",
    "    #If NOISE_RATE is greater than 0, noise will be added to the inputs.\n",
    "    \n",
    "    if add_noise:\n",
    "      sh = xs.shape\n",
    "      noise_mask = torch.bernoulli(torch.full(sh, NOISE_RATE))  # 0 (keep) or 1 (replace with noise)\n",
    "      #Gets the shape of the input tensor batch.\n",
    "      #Creates a binary mask using Bernoulli sampling, where each element has NOISE_RATE probability of being 1 \n",
    "      #(indicating where noise will be applied) and 1-NOISE_RATE probability of being 0.\n",
    "            \n",
    "      sp_noise = torch.bernoulli(torch.full(sh, 0.5))-0.5  # -1 or 1\n",
    "      #Generates the actual noise values as either -0.5 or 0.5.\n",
    "      #First creates a tensor of the same shape filled with 0.5, then applies Bernoulli \n",
    "      #sampling to get 0s or 1s.\n",
    "      #Subtracts 0.5 to convert to -0.5 or 0.5 (this creates salt and pepper noise).\n",
    "        \n",
    "      xns = xs * (1-noise_mask) + sp_noise * noise_mask\n",
    "      #Creates the noisy input xns by:\n",
    "          #Keeping original values where the mask is 0: xs * (1-noise_mask)\n",
    "          #Adding noise values where the mask is 1: sp_noise * noise_mask\n",
    "          #The result is a tensor where some values are preserved \n",
    "          #from the original input and others are replaced with noise.\n",
    "      \n",
    "      # sp = sp_noise\n",
    "    else:\n",
    "       xns = xs\n",
    "    #If no noise is to be added, the noisy input is the same as the original input.\n",
    "\n",
    "    return xns.to(device), xs.to(device), ys.to(device)\n",
    "    #Returns three tensors, all moved to the specified device (likely GPU):\n",
    "\n",
    "    #xns: The inputs with noise added (or original inputs if no noise)\n",
    "    #xs: The original clean inputs\n",
    "    #ys: The labels or targets\n",
    "    #\n",
    "    #\n",
    "    #This return structure is typical for denoising autoencoders, where you need \n",
    "    #both the noisy input (fed to the encoder) and the clean target \n",
    "    #(used to compute the reconstruction loss).\n",
    "    #\n",
    "    #This function is specifically designed for training denoising autoencoders, \n",
    "    #where the model learns to remove noise from corrupted inputs by trying \n",
    "    #to reconstruct the original clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgnSKMqodiiE"
   },
   "outputs": [],
   "source": [
    "# # given a AE model `model`\n",
    "# for img, label in valid_dataset:\n",
    "#     reconstruction = model(img)\n",
    "#     loss_value = loss(img, reconstruction).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbR9emfD8d41"
   },
   "source": [
    "## Helper Autoencoder Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNM2dEZOh5EN"
   },
   "source": [
    "We will start from implementing an Autoencoder model base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGs_gV5GtAc8"
   },
   "outputs": [],
   "source": [
    "\n",
    "def eval_on_samples(ae_model, epoch, samples):\n",
    "    # this is called on end of each training epoch\n",
    "    xns = samples['images_noisy']\n",
    "    xns = torch.tensor(xns, dtype=torch.float32).to(device)\n",
    "    #labels = samples['labels']\n",
    "\n",
    "# Function to evaluate the autoencoder on sample data after each epoch\n",
    "# Takes the model, current epoch number, and samples dictionary\n",
    "# Extracts noisy images from the samples and converts them to a PyTorch tensor on the target device\n",
    "# The labels are extracted but commented out (not used)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yz = ae_model(xns, return_z=True)\n",
    "        yz = [el.detach().cpu().numpy() for el in yz]\n",
    "\n",
    "        y = yz[0]\n",
    "        z = yz[1:]\n",
    "    # Uses torch.no_grad() to disable gradient calculation (for efficiency during evaluation)\n",
    "    # Gets both reconstructions and encodings (i.e. latent space!) by calling the model with return_z=True\n",
    "    # Converts all outputs to NumPy arrays\n",
    "    # Separates the reconstructions y and encodings z\n",
    "\n",
    "    res = {'z': z, 'y': y, 'epoch': epoch}\n",
    "    return res\n",
    "\n",
    "# Creates and returns a dictionary containing:\n",
    "\n",
    "# z: The encoded representations\n",
    "# y: The reconstructed images\n",
    "# epoch: The current epoch number\n",
    "# \n",
    "\n",
    "# This evaluation function captures the model's performance at each epoch, allowing for tracking reconstruction quality and analyzing the learned representations over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoITlVuntAc8"
   },
   "outputs": [],
   "source": [
    "def plot_hist(history, logscale=True):\n",
    "    \"\"\"\n",
    "    plot training loss\n",
    "    \"\"\"\n",
    "\n",
    "    loss = history['loss']\n",
    "    v_loss = history['val_loss']\n",
    "    epochs = history['epoch']\n",
    "\n",
    "    # This function visualizes training history (loss over epochs).\n",
    "    # Extracts training loss, validation loss, and epoch numbers from the history dictionary.\n",
    "\n",
    "    \n",
    "    plot = plt.semilogy if logscale else plt.plot\n",
    "    # Cleverly chooses between logarithmic scale (plt.semilogy) or linear scale (plt.plot) based on the logscale parameter.\n",
    "    # Default is logarithmic scale, which is often better for visualizing loss curves as they typically decrease exponentially.\n",
    "    \n",
    "    plot(epochs, loss, label='training');\n",
    "    plot(epochs, v_loss, label='validation');\n",
    "    # Plots both training and validation loss curves using the selected plotting function.\n",
    "    # Labels each curve for the legend.\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # Adds a legend, axis labels, displays the plot, and then closes the figure.\n",
    "\n",
    "\n",
    "\n",
    "def plot_samples(sample_history, samples, epoch_stride=5, fig_scale=1):\n",
    "    \"\"\"\n",
    "    Plots input, noisy samples (for DAE) and reconstruction.\n",
    "    Each `epoch_stride`-th epoch\n",
    "    \"\"\"\n",
    "    # This function visualizes sample reconstructions over training epochs.\n",
    "    # Shows how the model's reconstruction capability improves over time.\n",
    "\n",
    "    single_el_idx = samples['single_el_idx']\n",
    "    images_noisy = samples['images_noisy'][single_el_idx, 0]\n",
    "    images = samples['images'][single_el_idx, 0]\n",
    "    # Extracts indices for selected samples to visualize.\n",
    "    # Gets the noisy input images and the original clean images for these samples.\n",
    "    # The , 0 indexing suggests selecting the first channel of each image.\n",
    "\n",
    "    last_epoch = np.max(list(sample_history.keys()))\n",
    "    # Determines the last epoch number in the history data.\n",
    "\n",
    "    for epoch_idx, hist_el in sample_history.items():\n",
    "      if epoch_idx % epoch_stride != 0 and epoch_idx != last_epoch:\n",
    "        continue\n",
    "    # Iterates through each epoch's results in the history.\n",
    "    # Uses epoch_stride to select only every nth epoch (to avoid too many visualizations).\n",
    "    # Always includes the last epoch regardless of the stride.\n",
    "\n",
    "      samples_arr = [images_noisy, hist_el['y'][single_el_idx, 0], images]\n",
    "    # Creates an array of three sets of images to visualize side by side:\n",
    "       # The noisy input images\n",
    "       # The model's reconstructions for the current epoch\n",
    "       # The original clean images (ground truth)\n",
    "\n",
    "      ny = len(samples_arr)\n",
    "      nx = len(samples_arr[0])\n",
    "\n",
    "      plt.figure(figsize=(fig_scale*nx, fig_scale*ny))\n",
    "      # Calculates the dimensions of the visualization grid.\n",
    "      # Creates a figure with size proportional to the number of samples.\n",
    "\n",
    "        \n",
    "      m = mosaic(samples_arr)\n",
    "      # Uses the previously defined mosaic function to create a grid of all images.\n",
    "\n",
    "      plt.title(f'after epoch {int(epoch_idx)}')\n",
    "      plt.imshow(m, cmap='gray', vmin=-.5, vmax=.5)\n",
    "      # Adds a title showing which epoch this visualization represents.\n",
    "      # Displays the mosaic with a grayscale colormap and fixed value range.\n",
    "      # The vmin=-.5, vmax=.5 matches the normalized data range we've seen before.\n",
    "\n",
    "        \n",
    "      plt.tight_layout(pad=0.1, h_pad=0, w_pad=0)\n",
    "      plt.show()\n",
    "      plt.close()\n",
    "      # Ensures proper spacing in the figure.\n",
    "      # Displays the figure and then closes it to free memory.\n",
    "\n",
    "# This function creates a powerful visualization showing the progression of the model's reconstruction ability across epochs. Each visualization has three rows:\n",
    "# \n",
    "# The noisy inputs\n",
    "# The model's reconstructions\n",
    "# The original clean images (targets)\n",
    "# \n",
    "# This makes it easy to see how the model gradually learns to denoise and reconstruct the images over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwnutofatAc9"
   },
   "outputs": [],
   "source": [
    "# These are utility functions for working with trained models at different stages of training. Let me break them down:\n",
    "\n",
    "def run_on_trained(model, root_dir, run_fn, ep=None, model_filename=None):\n",
    "    \"\"\"\n",
    "    Helper function to excecute any function on model in state after `ep` training epoch\n",
    "    \"\"\"\n",
    "    # This function loads a model checkpoint and runs a specified function on it.\n",
    "    # Parameters:\n",
    "    # \n",
    "    # model: The neural network model instance\n",
    "    # root_dir: Directory containing saved model checkpoints\n",
    "    # run_fn: The function to run on the loaded model\n",
    "    # ep: Specific epoch to load (optional)\n",
    "    # model_filename: Specific checkpoint file to load (optional)\n",
    "\n",
    "    if model_filename is None:\n",
    "        if ep is not None:\n",
    "            model_filename = root_dir/f'model_{ep:03d}.pth'\n",
    "        else:\n",
    "            model_filename = sorted(list(root_dir.glob('*.pth')))[-1]  # last model state\n",
    "    # Determines which model checkpoint file to load:\n",
    "    # \n",
    "    # If a specific filename is provided, use that (in this case this code block would be skipped)\n",
    "    # If an epoch number is provided, construct the filename using a pattern\n",
    "    # If neither is provided, use the last checkpoint file (by alphabetical sorting)\n",
    "    # The code uses pathlib's Path objects for file handling (using / for path joining)\n",
    "\n",
    "    \n",
    "    model_dict = torch.load(model_filename,weights_only=False)\n",
    "\n",
    "    model.load_state_dict(model_dict['model_state_dict'])\n",
    "\n",
    "    # Loads the saved model state from the specified file\n",
    "    # The weights_only=False parameter indicates to load the full state dictionary (not just weights)\n",
    "    # Restores the model parameters from the saved state dictionary\n",
    "    \n",
    "\n",
    "    run_fn(model)\n",
    "    # Calls the provided function on the loaded model\n",
    "\n",
    "def run_on_all_training_history(model, root_dir, run_fn, n_ep=None):\n",
    "    \"\"\"\n",
    "    Helper function to excecute any function on model state after each of the training epochs\n",
    "    \"\"\"\n",
    "    # This function runs a specified function on multiple model checkpoints from different training epochs.\n",
    "    # Parameters:\n",
    "    # \n",
    "    # model: The neural network model instance\n",
    "    # root_dir: Directory containing saved model checkpoints\n",
    "    # run_fn: The function to run on each loaded model state\n",
    "    # n_ep: Specific number of epochs to process (optional)\n",
    "    \n",
    "    if n_ep is not None:\n",
    "        for ep in range(n_ep):\n",
    "            print(f'running on epoch {ep+1}/{n_ep}...')\n",
    "            run_on_trained(model, root_dir, run_fn, ep=ep)\n",
    "    # If a specific number of epochs is provided:\n",
    "    # \n",
    "    # Iterates through each epoch from 0 to n_ep-1\n",
    "    # Prints progress information\n",
    "    # Calls run_on_trained for each epoch\n",
    "    \n",
    "    else:\n",
    "        for model_filename in sorted(root_dir.glob('*.pth')):\n",
    "            print(f'running on checkpoint {model_filename}...')\n",
    "            run_on_trained(model, root_dir, run_fn, model_filename=model_filename)\n",
    "\n",
    "    # If no specific number of epochs is provided:\n",
    "    # \n",
    "    # Finds all .pth files in the root directory\n",
    "    # Sorts them (presumably by name, which would be by epoch if using the naming pattern)\n",
    "    # Processes each checkpoint file in order\n",
    "    \n",
    "    print(f'done')\n",
    "\n",
    "    # Prints a completion message when all checkpoints have been processed\n",
    "    # \n",
    "    # These utility functions make it easy to:\n",
    "    # \n",
    "    # Analyze a model at a specific point in its training history\n",
    "    # Run the same analysis across multiple stages of training\n",
    "    # Visualize or evaluate how the model's behavior changes over the course of training\n",
    "    # \n",
    "    # They're particularly useful for post-training analysis, debugging, and creating visualizations of model evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xns.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path('/Users/stephanehess/Documents/CAS_AML/autoencoder_tutorials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = root_path/\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_without_person = pd.read_csv(image_dir/'with_without_person_mod.csv')\n",
    "with_without_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_labels_idx(label_data, image_ids):\n",
    "    labels = []\n",
    "    for image_id in image_ids:\n",
    "        selection_bools = label_data.image_id == image_id\n",
    "        idx_label = label_data[selection_bools].recognisable.iloc[0]\n",
    "        labels.append(int(idx_label))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process images and split into training/validation sets\n",
    "train_images, val_images, train_ids, val_ids = lpi.load_process_images(image_dir)\n",
    "\n",
    "# Print shapes and identifiers\n",
    "print(f\"Training data shape: {train_images.shape}\")\n",
    "print(f\"Validation data shape: {val_images.shape}\")\n",
    "print(f\"Training identifiers: {train_ids}\")\n",
    "print(f\"Validation identifiers: {val_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get smaller copy of data set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ims_small = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_small = train_images[0:num_ims_small].copy()\n",
    "#val_images_small = val_images[0:num_ims_small].copy()\n",
    "val_images_small = train_images_small.copy() \n",
    "train_ids_small = train_ids[0:num_ims_small].copy()\n",
    "#val_ids_small = val_ids[0:num_ims_small].copy()\n",
    "val_ids_small = train_ids_small.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the small data set training and validation set are identical for the following purposes:\n",
    "1. To test if the algorithm learns anything at all.\n",
    "2. To test if clustering with a limited data set is possible. This means that the model will be completely overfitted and not generalizable at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add channel dimension to image data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)\n",
    "print(val_images.shape)\n",
    "print(train_images_small.shape)\n",
    "print(val_images_small.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_small_n = train_images_small[:, np.newaxis, :, :]\n",
    "val_images_small_n = val_images_small[:, np.newaxis, :, :]\n",
    "print(train_images_small_n.shape)\n",
    "print(val_images_small_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_n = train_images[:, np.newaxis, :, :]\n",
    "val_images_n = val_images[:, np.newaxis, :, :]\n",
    "print(train_images_n.shape)\n",
    "print(val_images_n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load label data and reconvert image ids: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_person = pd.read_csv(image_dir/'with_without_person_mod.csv')\n",
    "labels_person.image_id = img_idc.reconvert_image_ids(labels_person.image_id)\n",
    "labels_person.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_person.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the corresponding for each image in train and val data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_small = match_labels_idx(labels_person, train_ids_small)\n",
    "val_labels_small = match_labels_idx(labels_person, val_ids_small)\n",
    "print(val_labels_small[0:3])\n",
    "print(train_labels_small[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = match_labels_idx(labels_person, train_ids)\n",
    "val_labels = match_labels_idx(labels_person, val_ids)\n",
    "print(val_labels[0:3])\n",
    "print(train_labels[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make new indices for train images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_indices_train_small = list(range(0, len(train_images_small_n)))\n",
    "new_indices_train_small[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_indices_train = list(range(0, len(train_images_n)))\n",
    "new_indices_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_img_small = {\n",
    "    'images_noisy': train_images_small_n,\n",
    "    'images': train_images_small_n,\n",
    "    'labels': train_labels_small,\n",
    "    'single_el_idx': new_indices_train_small,\n",
    "    'image_ids': train_ids_small\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_img = {\n",
    "    'images_noisy': train_images_n,\n",
    "    'images': train_images_n,\n",
    "    'labels': train_labels,\n",
    "    'single_el_idx': new_indices_train,\n",
    "    'image_ids': train_ids\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = 0.5, 1.\n",
    "# m, s = 0.5, 0.5\n",
    "#m, s = 0., 1.\n",
    "\n",
    "#Defines normalization parameters for the images: mean (m) and standard deviation (s).\n",
    "#The active values are m=0.5 and s=1.0.\n",
    "#The commented lines show alternative normalization parameters that were tried.\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Pad(2), # to make images 32x32\n",
    "    transforms.Normalize((m,), (s,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_tensor(image_array):\n",
    "    tensor_list = []\n",
    "    for idx in range(image_array.shape[0]):\n",
    "        image_tensor_idx = transform(image_array[idx])\n",
    "        tensor_list.append(image_tensor_idx)\n",
    "\n",
    "    images_tensor = torch.stack(tensor_list)\n",
    "    return images_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_tensor = array_to_tensor(train_images_small)\n",
    "val_images_tensor = array_to_tensor(val_images_small)\n",
    "print(type(train_images_tensor))\n",
    "print(len(train_images_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_length = train_images_tensor.shape[-1]\n",
    "image_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_length/(2**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_l = 17\n",
    "#hidden_fs = 34\n",
    "my_hidden_size = (hidden_l**2)\n",
    "my_hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, code_size):\n",
    "        self.input_size = list(input_size)  # shape of data sample\n",
    "        self.flat_data_size = np.prod(self.input_size)\n",
    "        self.hidden_size = my_hidden_size\n",
    "\n",
    "        self.code_size = code_size  # code size\n",
    "\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        #Creates an autoencoder neural network that inherits from PyTorch's nn.Module.\n",
    "        #Takes two parameters:\n",
    "        #\n",
    "        #input_size: The shape of input data (e.g., [1, 28, 28] for MNIST)\n",
    "        #code_size: The dimension of the encoded representation (bottleneck)\n",
    "        #\n",
    "        #\n",
    "        #Calculates the flattened input size by multiplying all dimensions.\n",
    "        #Sets an intermediate hidden layer size of 128 neurons.\n",
    "        #Calls the parent class initializer.\n",
    "\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(self.flat_data_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.hidden_size, self.hidden_size*2//3),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.hidden_size*2//3, self.hidden_size*4//9),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.hidden_size*4//9, self.code_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        #Defines the encoder network as a sequence of operations:\n",
    "            #\n",
    "            #Flattens the input (e.g., converts a 2D image to 1D)\n",
    "            #Linear layer mapping from input size to hidden size\n",
    "            #ReLU activation\n",
    "            #Linear layer mapping from hidden size to code size\n",
    "            #Sigmoid activation (constrains the encoded values to [0, 1])\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.code_size, self.hidden_size*4//9),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.hidden_size*4//9, self.hidden_size*2//3),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.hidden_size*2//3, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.hidden_size, self.flat_data_size),\n",
    "            nn.Tanh(),  # Think: why tanh?\n",
    "\n",
    "            nn.Unflatten(1, self.input_size),\n",
    "        )\n",
    "        #Defines the decoder network:\n",
    "            #Linear layer from code size to hidden size\n",
    "            #ReLU activation\n",
    "            #Linear layer from hidden size back to the flattened input size\n",
    "            #Tanh activation (outputs values in [-1, 1], matching the normalized input range)\n",
    "            #Unflattens the output back to the original input shape\n",
    "\n",
    "#Regarding \"why tanh?\": Tanh is used because the input images were normalized to approximately [-0.5, 0.5] \n",
    "    #range (using m=0.5, s=1.0). Tanh outputs values in the range [-1, 1], \n",
    "    #which after scaling by 1.1 in the decode method closely matches the input data range.\n",
    "\n",
    "    def forward(self, x, return_z=False):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return (decoded, encoded) if return_z else decoded\n",
    "    # The forward pass:\n",
    "        #Encodes the input\n",
    "        #Decodes the encoded representation\n",
    "        #If return_z=True, returns both the reconstruction and the encoded values\n",
    "        #Otherwise, just returns the reconstruction\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)*1.1\n",
    "# Helper methods to encode and decode separately\n",
    "# Note the multiplication by 1.1 in the decode method, \n",
    "    # which slightly amplifies the output range to better match the input data distribution\n",
    "\n",
    "        \n",
    "\n",
    "    def get_n_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    # Utility method to count the total number of trainable parameters in the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_SIZE = 50\n",
    "NOISE_RATE = 0\n",
    "MODEL_NAME = 'ae_model'\n",
    "in_size = train_images_tensor.shape[1:]\n",
    "model = AutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
    "train_images_tensor = train_images_tensor.to(device)\n",
    "val_images_tensor = val_images_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the autoencoder model, for N_EPOCHS epochs,\n",
    "# save history of loss values for training and validation sets,\n",
    "# history of validation samples evolution, and model weights history,\n",
    "\n",
    "N_EPOCHS = 185\n",
    "LR = 0.0004\n",
    "\n",
    "\n",
    "model_root = pl.Path(MODEL_NAME)\n",
    "model_root.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# implement loss explicitly\n",
    "loss = nn.L1Loss()\n",
    "\n",
    "# train the model\n",
    "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
    "sample_history = {}\n",
    "\n",
    "pbar = tqdm.tqdm(range(0, N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
    "for epoch_idx in pbar:\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    #for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_images_tensor)\n",
    "    loss_value = loss(output, train_images_tensor)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    epoch_loss += loss_value.detach().cpu().item()\n",
    "    \n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['epoch'].append(epoch_idx)\n",
    "    # update progress bar\n",
    "\n",
    "    # evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        #for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
    "        \n",
    "        output = model(val_images_tensor)\n",
    "        loss_value = loss(output, val_images_tensor)\n",
    "        val_loss += loss_value.detach().cpu().item()\n",
    "        \n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "    pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
    "    # evaluate on samples\n",
    "    sample_res = eval_on_samples(model, epoch_idx, samples=samples_img_small)\n",
    "    sample_history[epoch_idx] = sample_res\n",
    "\n",
    "    # save model weights\n",
    "    torch.save({\n",
    "                'epoch': epoch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss\n",
    "                }, model_root/f'model_{epoch_idx:03d}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_el_idx = samples_img_small['single_el_idx']\n",
    "#plot_im_samples(samples_img_small['images_noisy'][single_el_idx, 0], n=2, is_torch=False)\n",
    "plot_im_samples(samples_img_small['images'][single_el_idx, 0], n=10, is_torch=False)\n",
    "plot_im_samples(sample_history[174]['y'][single_el_idx, 0], n=10, is_torch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_img_small.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_img_small['image_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample_history[29]['z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample_history[29]['z'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_history[124]['z'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space = sample_history[124]['z'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(perplexity= 1, init=\"pca\", learning_rate=\"auto\", random_state=42)\n",
    "X_valid_2D = tsne.fit_transform(latent_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], s=10, cmap=\"tab10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_valid_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example array with shape (10, 2)\n",
    "# Replace this with your actual data\n",
    "data = X_valid_2D.copy()\n",
    "\n",
    "\n",
    "inertia = []\n",
    "k_range = range(1, 10)  # Test from 1 to 9 clusters\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "    kmeans.fit(data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, inertia, 'o-', color='blue')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Specify the number of clusters (k)\n",
    "# You can adjust this based on your needs\n",
    "k = 5\n",
    "\n",
    "# Initialize and fit the k-means model\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Get cluster assignments for each data point\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Get the coordinates of the cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Print results\n",
    "print(\"Cluster assignments:\", labels)\n",
    "print(\"Cluster centers:\", centers)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', s=100, alpha=0.7)\n",
    "#plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "#plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200)\n",
    "plt.title('K-means Clustering Results')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the silhouette score to evaluate the clustering quality\n",
    "from sklearn.metrics import silhouette_score\n",
    "if k > 1 and len(np.unique(labels)) > 1:  # Ensure there are at least 2 clusters with data\n",
    "    score = silhouette_score(data, labels)\n",
    "    print(f\"Silhouette Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# Specify the number of clusters (k)\n",
    "k = 5\n",
    "\n",
    "# Initialize and fit the k-means model\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Get cluster assignments for each data point\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Get the coordinates of the cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Print results\n",
    "print(\"Cluster assignments:\", labels)\n",
    "print(\"Cluster centers:\", centers)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', s=100, alpha=0.7)\n",
    "\n",
    "# Create custom legend handles\n",
    "legend_handles = []\n",
    "for i in range(k):\n",
    "    color = scatter.cmap(scatter.norm(i))\n",
    "    patch = mpatches.Patch(color=color, label=f'Cluster {i}')\n",
    "    legend_handles.append(patch)\n",
    "\n",
    "plt.title('K-means Clustering Results')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend(handles=legend_handles)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the silhouette score to evaluate the clustering quality\n",
    "from sklearn.metrics import silhouette_score\n",
    "if k > 1 and len(np.unique(labels)) > 1:  # Ensure there are at least 2 clusters with data\n",
    "    score = silhouette_score(data, labels)\n",
    "    print(f\"Silhouette Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_im_samples(ds, numbers=None, n=5, is_torch=False):\n",
    "    fig, axs = plt.subplots(1, n, figsize=(16, n))\n",
    "    \n",
    "    for i, image in enumerate(ds[:n]):\n",
    "        axs[i].imshow(to_np_showable(image) if is_torch else image, cmap='gray')\n",
    "        \n",
    "        # Add title if numbers array is provided\n",
    "        if numbers is not None and i < len(numbers):\n",
    "            axs[i].set_title(str(numbers[i]), fontsize=12)\n",
    "            \n",
    "        axs[i].set_axis_off()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_el_idx = samples_img_small['single_el_idx']\n",
    "#plot_im_samples(samples_img_small['images_noisy'][single_el_idx, 0], n=2, is_torch=False)\n",
    "plot_im_samples(samples_img_small['images'][single_el_idx, 0], labels, n=10, is_torch=False)\n",
    "plot_im_samples(sample_history[174]['y'][single_el_idx, 0], labels, n=10, is_torch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_tensor = array_to_tensor(train_images)\n",
    "val_images_tensor = array_to_tensor(val_images)\n",
    "print(type(train_images_tensor))\n",
    "print(len(train_images_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_length = train_images_tensor.shape[-1]\n",
    "image_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_length/(2**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_l = 9\n",
    "hidden_fs = 64\n",
    "my_hidden_size = (hidden_l**2)*hidden_fs\n",
    "my_hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hidden_size//8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, code_size):\n",
    "        self.input_size = list(input_size)  # shape of data sample\n",
    "        self.flat_data_size = np.prod(self.input_size)\n",
    "        self.hidden_size = my_hidden_size\n",
    "\n",
    "        self.code_size = code_size  # code size\n",
    "\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        #Creates an autoencoder neural network that inherits from PyTorch's nn.Module.\n",
    "        #Takes two parameters:\n",
    "        #\n",
    "        #input_size: The shape of input data (e.g., [1, 28, 28] for MNIST)\n",
    "        #code_size: The dimension of the encoded representation (bottleneck)\n",
    "        #\n",
    "        #\n",
    "        #Calculates the flattened input size by multiplying all dimensions.\n",
    "        #Sets an intermediate hidden layer size of 128 neurons.\n",
    "        #Calls the parent class initializer.\n",
    "\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(self.flat_data_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.hidden_size, self.code_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        #Defines the encoder network as a sequence of operations:\n",
    "            #\n",
    "            #Flattens the input (e.g., converts a 2D image to 1D)\n",
    "            #Linear layer mapping from input size to hidden size\n",
    "            #ReLU activation\n",
    "            #Linear layer mapping from hidden size to code size\n",
    "            #Sigmoid activation (constrains the encoded values to [0, 1])\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.code_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.hidden_size, self.flat_data_size),\n",
    "            nn.Tanh(),  # Think: why tanh?\n",
    "\n",
    "            nn.Unflatten(1, self.input_size),\n",
    "        )\n",
    "        #Defines the decoder network:\n",
    "            #Linear layer from code size to hidden size\n",
    "            #ReLU activation\n",
    "            #Linear layer from hidden size back to the flattened input size\n",
    "            #Tanh activation (outputs values in [-1, 1], matching the normalized input range)\n",
    "            #Unflattens the output back to the original input shape\n",
    "\n",
    "#Regarding \"why tanh?\": Tanh is used because the input images were normalized to approximately [-0.5, 0.5] \n",
    "    #range (using m=0.5, s=1.0). Tanh outputs values in the range [-1, 1], \n",
    "    #which after scaling by 1.1 in the decode method closely matches the input data range.\n",
    "\n",
    "    def forward(self, x, return_z=False):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return (decoded, encoded) if return_z else decoded\n",
    "    # The forward pass:\n",
    "        #Encodes the input\n",
    "        #Decodes the encoded representation\n",
    "        #If return_z=True, returns both the reconstruction and the encoded values\n",
    "        #Otherwise, just returns the reconstruction\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)*1.1\n",
    "# Helper methods to encode and decode separately\n",
    "# Note the multiplication by 1.1 in the decode method, \n",
    "    # which slightly amplifies the output range to better match the input data distribution\n",
    "\n",
    "        \n",
    "\n",
    "    def get_n_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    # Utility method to count the total number of trainable parameters in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoEncoder(AutoEncoder):\n",
    "# This class inherits from the AutoEncoder base class we discussed earlier, \n",
    "# extending it to use convolutional layers instead of fully connected layers.\n",
    "    def __init__(self, input_size, code_size):\n",
    "        self.input_size = list(input_size)  # shape of data sample\n",
    "\n",
    "        self.hidden_size = my_hidden_size\n",
    "\n",
    "        self.code_size = code_size  # code size\n",
    "\n",
    "        super(ConvolutionalAutoEncoder, self).__init__(input_size, code_size)\n",
    "        # Initializes with the same parameters as the base class\n",
    "        # Sets hidden_size to 128 (32Ã2Ã2), which will be the size of the flattened representation before the final encoding\n",
    "        # Calls the parent class initializer, but will override the encoder and decoder definitions\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1,   hidden_fs//(2**3), 3, padding=1, stride=1), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(hidden_fs//(2**3),   hidden_fs//(2**3), 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(hidden_fs//(2**3),  hidden_fs//(2**2), 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(hidden_fs//(2**2), hidden_fs//(2**2), 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(hidden_fs//(2**2), hidden_fs//(2**1), 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(hidden_fs//(2**1), hidden_fs//(2**1), 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            #nn.Conv2d(hidden_fs//(2**1), hidden_fs//(2**1), 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(hidden_fs//(2**1), hidden_fs, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "\n",
    "            # Defines a series of convolutional layers for the encoder\n",
    "            # Each layer increases the number of feature maps while reducing spatial dimensions\n",
    "            # Starting with 1 channel (grayscale input), increasing to 8, then 16, then 32 channels\n",
    "            # Uses padding=1 to maintain spatial dimensions before downsampling\n",
    "            # Uses stride=2 in most layers to perform downsampling (reducing spatial dimensions by half)\n",
    "            # Each convolution is followed by LeakyReLU activation with negative_slope=0.3\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(self.hidden_size, self.hidden_size//8), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Linear(self.hidden_size//8, self.code_size),\n",
    "            # nn.Tanh(),\n",
    "            # After the convolutional layers, flattens the 3D feature maps into a 1D vector\n",
    "            # Applies two fully connected layers to reduce dimensions to the final code_size\n",
    "            # The final Tanh activation is commented out\n",
    "            \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.code_size, self.hidden_size), nn.LeakyReLU(negative_slope=0.3),\n",
    "\n",
    "            nn.Unflatten(1, (hidden_fs, hidden_l, hidden_l)),\n",
    "            # The decoder starts with a fully connected layer to expand from code_size to hidden_size\n",
    "            # Reshapes the 1D vector back to a 3D feature map with shape (32, 2, 2)\n",
    "\n",
    "            nn.ConvTranspose2d(hidden_fs, hidden_fs//(2**1), 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            #nn.ConvTranspose2d(hidden_fs//(2**1), hidden_fs//(2**1), 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.ConvTranspose2d(hidden_fs//(2**1), hidden_fs//(2**1), 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.ConvTranspose2d(hidden_fs//(2**1), hidden_fs//(2**1), 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.ConvTranspose2d(hidden_fs//(2**1), hidden_fs//(2**1), 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.ConvTranspose2d(hidden_fs//(2**1),  hidden_fs//(2**2), 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.ConvTranspose2d(hidden_fs//(2**2),   hidden_fs//(2**2), 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "\n",
    "            # Uses transposed convolutions (also called deconvolutions) to upsample the feature maps\n",
    "            # Mirrors the encoder structure in reverse, gradually decreasing channels and increasing spatial dimensions\n",
    "            # output_padding=1 helps ensure the spatial dimensions match the original input after upsampling\n",
    "            \n",
    "            nn.Conv2d(hidden_fs//(2**2), 1, 3, padding=1, stride=1), nn.Tanh(),\n",
    "            # Final convolution layer produces a single channel output (grayscale image)\n",
    "            # Tanh activation constrains the output values to [-1, 1] range\n",
    "        )\n",
    "\n",
    "    def decode(self, z):\n",
    "        reconstruction = self.decoder(z)\n",
    "        reconstruction = reconstruction[:, :, 2:-2, 2:-2]\n",
    "        return reconstruction\n",
    "\n",
    "        # Overrides the parent class's decode method\n",
    "        # Applies the decoder to the latent representation\n",
    "        # Crops the reconstructed image by removing 2 pixels from each side\n",
    "        # This cropping likely compensates for any dimension mismatches caused by the convolution/deconvolution operations\n",
    "\n",
    "# This convolutional architecture is much more powerful for image data than the fully connected version, as it:\n",
    "# \n",
    "# 1. Preserves spatial relationships in the data\n",
    "# 2. Uses parameter sharing for efficiency\n",
    "# 3. Can learn hierarchical features (edges, textures, patterns)\n",
    "# 4. Generally results in better reconstructions and more meaningful latent representations for image data\n",
    "# \n",
    "# The progression from a simple fully connected autoencoder to this convolutional version is a common step in improving deep learning models for image data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_SIZE = 50\n",
    "MODEL_NAME = 'cdae_model'\n",
    "in_size = train_images_tensor.shape[1:]\n",
    "model = ConvolutionalAutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
    "train_images_tensor = train_images_tensor.to(device)\n",
    "val_images_tensor = val_images_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchinfo import summary\n",
    "#summary(model, input_size=(1, 1, 572, 572))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the autoencoder model, for N_EPOCHS epochs,\n",
    "# save history of loss values for training and validation sets,\n",
    "# history of validation samples evolution, and model weights history,\n",
    "\n",
    "N_EPOCHS = 35\n",
    "LR = 0.0004\n",
    "\n",
    "\n",
    "model_root = pl.Path(MODEL_NAME)\n",
    "model_root.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# implement loss explicitly\n",
    "loss = nn.L1Loss()\n",
    "\n",
    "# train the model\n",
    "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
    "sample_history = {}\n",
    "\n",
    "pbar = tqdm.tqdm(range(0, N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
    "for epoch_idx in pbar:\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    #for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_images_tensor)\n",
    "    loss_value = loss(output, train_images_tensor)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    epoch_loss += loss_value.detach().cpu().item()\n",
    "    \n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['epoch'].append(epoch_idx)\n",
    "    # update progress bar\n",
    "\n",
    "    # evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        #for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
    "        \n",
    "        output = model(val_images_tensor)\n",
    "        loss_value = loss(output, val_images_tensor)\n",
    "        val_loss += loss_value.detach().cpu().item()\n",
    "        \n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "    pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
    "    # evaluate on samples\n",
    "    sample_res = eval_on_samples(model, epoch_idx, samples=samples_img)\n",
    "    sample_history[epoch_idx] = sample_res\n",
    "\n",
    "    # save model weights\n",
    "    torch.save({\n",
    "                'epoch': epoch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss\n",
    "                }, model_root/f'model_{epoch_idx:03d}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_img['images_noisy'][0:3, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_img['images_noisy'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_el_idx = samples_img['single_el_idx']\n",
    "plot_im_samples(samples_img['images_noisy'][single_el_idx, 0], n=20, is_torch=False)\n",
    "plot_im_samples(samples_img['images'][single_el_idx, 0], n=20, is_torch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Jupyter notebook cell magic to suppress output (useful for code that might produce verbose output)\n",
    "\n",
    "single_el_idx = samples_img['single_el_idx']\n",
    "images_noisy = samples_img['images_noisy'][single_el_idx, 0]\n",
    "images = samples_img['images'][single_el_idx, 0]\n",
    "# Extracts the indices for the selected sample images\n",
    "# Gets the noisy input images and original clean images for these indices\n",
    "# The , 0 selects the first channel (since these are grayscale images)\n",
    "\n",
    "smpl_ims = []\n",
    "for epoch_idx, hist_el in sample_history.items():\n",
    "    samples_arr = [images_noisy, hist_el['y'][single_el_idx, 0], images]\n",
    "    smpl_ims.append(samples_arr)\n",
    "# Creates a list to store image arrays for each epoch\n",
    "# For each epoch in the training history:\n",
    "    # \n",
    "    # Creates an array containing [noisy inputs, reconstructions, original images]\n",
    "    # Adds this array to the list\n",
    "\n",
    "ny, nx = len(smpl_ims[0]), len(smpl_ims[0][0])\n",
    "# Determines the number of rows (3: noisy, reconstructed, original) and columns (number of samples)\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above animations use JavaScript\n",
    "# Sets matplotlib to use JavaScript for HTML animations in Jupyter\n",
    "\n",
    "s=1\n",
    "fig = plt.figure(figsize=(s*nx, s*ny))\n",
    "# Creates a figure with size proportional to the number of images\n",
    "\n",
    "m = mosaic(smpl_ims[0])\n",
    "\n",
    "ttl = plt.title(f'after epoch {int(0)}')\n",
    "# plot 0th epoch - 0th frame\n",
    "imsh = plt.imshow(m, cmap='gray', vmin=-0.5, vmax=0.5)\n",
    "# Creates the initial frame of the animation using the first epoch's images\n",
    "# Uses the mosaic function to arrange the images in a grid\n",
    "# Sets grayscale colormap with value range [-0.5, 0.5]\n",
    "\n",
    "# this function will be called to render each of the frames\n",
    "def animate(i):\n",
    "    m = mosaic(smpl_ims[i])\n",
    "    imsh.set_data(m)\n",
    "\n",
    "    ttl.set_text(f'after epoch {i}')\n",
    "\n",
    "    return imsh\n",
    "\n",
    "# Defines a function to update the plot for each frame of the animation\n",
    "# Creates a mosaic of images for the current epoch\n",
    "# Updates the image data and title text\n",
    "# Returns the updated image object\n",
    "\n",
    "# create animation\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(smpl_ims))\n",
    "\n",
    "# Creates an animation that calls the animate function for each epoch\n",
    "# The result is a dynamic visualization showing how reconstructions evolve throughout training\n",
    "\n",
    "# This animation provides an intuitive way to observe the autoencoder's learning \n",
    "# progress, allowing you to see how the model gradually improves at reconstructing \n",
    "# the original images from the noisy inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above, uses JavaScript\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "# Suppresses output with %%capture\n",
    "# Sets matplotlib to use JavaScript for HTML animations\n",
    "# Creates a square figure with size 8Ã8 inches\n",
    "\n",
    "labels = samples_img['labels']\n",
    "epochs = sorted(sample_history.keys())\n",
    "z_res = [sample_history[ep]['z'][0] for ep in epochs]\n",
    "# Gets the digit labels from the samples dictionary\n",
    "# Creates a sorted list of all epoch numbers\n",
    "# Extracts the latent space representations from each epoch\n",
    "\n",
    "scat = plt.scatter(z_res[0][:,0], z_res[0][:,1], c=labels, cmap=cm.rainbow)\n",
    "# Creates a scatter plot using the first two dimensions of the latent space from the first epoch\n",
    "# Colors the points according to their digit labels (0-9)\n",
    "# Uses the rainbow colormap to distinguish between different digits\n",
    "\n",
    "plt.xlim(-6.1, 6.1)\n",
    "plt.ylim(-6.1, 6.1)\n",
    "\n",
    "ax = plt.gca()\n",
    "legend1 = ax.legend(*scat.legend_elements(), title=\"digits\")\n",
    "ax.add_artist(legend1)\n",
    "ax.set_aspect('equal')\n",
    "ttl = plt.title(f'after epoch {0}')\n",
    "# Sets fixed axis limits for consistent visualization across frames\n",
    "# Gets the current axis\n",
    "# Adds a legend showing the mapping between colors and digit classes\n",
    "# Sets the aspect ratio to equal so circles appear as circles\n",
    "# Adds a title showing the current epoch\n",
    "\n",
    "def animate(i):\n",
    "    z = z_res[i]\n",
    "    scat.set_offsets(z)\n",
    "    ttl.set_text(f'after epoch {i}')\n",
    "    return scat\n",
    "\n",
    "# Defines a function to update the plot for each animation frame\n",
    "# Updates the scatter plot with the latent representations from the current epoch\n",
    "# Updates the title text with the current epoch number\n",
    "# Returns the updated scatter plot object\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(z_res))\n",
    "\n",
    "# Creates an animation that runs through all epochs\n",
    "\n",
    "# This animation shows how the model progressively learns to organize \n",
    "# the latent space, with points representing the same digit class gradually \n",
    "# clustering together. It's a powerful visualization that helps understand \n",
    "# how the autoencoder is learning meaningful representations and \n",
    "# separating different classes in the latent space, even though it's \n",
    "# trained in an unsupervised manner without using the labels for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGdgxx9LngGC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "smjlo1x48N3t",
    "TVG7kpjf8T2D",
    "cbR9emfD8d41",
    "9QH3J3Z-jTb2",
    "7-YdzZ2QDwLg",
    "4saA2FcDFcMq",
    "tu5h3HaT6aCN",
    "AVUXN5Jq7kx9",
    "9qArUdx6YHzD",
    "Tk1sA3cNFuLB",
    "phTJlLGx3a5H",
    "8_tWAYpah3aG",
    "9PLAxq3XIho_",
    "lgVvSoXa5vOk",
    "G3DrSyps5wWv",
    "kHPnzzd_6PLO",
    "YVruUa0d7fMw",
    "G1_R6CnAy7-F"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00e2ab4b7e60439aa1bd61bdf4781bb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f5f98a52e824aeb921b99cb6f3527bc",
       "IPY_MODEL_c4f4e17e87904108b4442cdb36443429",
       "IPY_MODEL_f3e816556c95470794f40759a8d8c58e"
      ],
      "layout": "IPY_MODEL_508a0fc496f445dbb37a177c3b2ff4bf"
     }
    },
    "03b115e9fd9e4f6bacee75bf1e91735f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05eed87241de4972a7acc70f48c3430c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0d97d53e4453459ab1ed6f7e49c1d6d6",
       "IPY_MODEL_7e7868cebe47479e82ea30763c86edca",
       "IPY_MODEL_d607db3bc256454092cb1eb024901c20"
      ],
      "layout": "IPY_MODEL_f03adde2b61040daa05a9a981512af51"
     }
    },
    "0676ba35b6194a078f53b30ad05f4991": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d97d53e4453459ab1ed6f7e49c1d6d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb197b7ef6d6484e80a4f77f6f03b1e7",
      "placeholder": "â",
      "style": "IPY_MODEL_8bc1b3f02e3543989bacf0aa882589e0",
      "value": "Generatingâvalidâsplit:â"
     }
    },
    "0da5b7e0ce25441cba444e632ef62340": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0fd05eefe82d4e63b9fda9a6124e5153": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fedb0e6df744ad7b32dd5f14a493b61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1179f40122794b168dc0665d77b3bd15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1623c03a91fa4f80b9109e281bda1a32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b321467d620a4a908cd906a6c6d28a17",
       "IPY_MODEL_fae564eac5eb4d95a5fda6b3e6b083de",
       "IPY_MODEL_c705c3f1b21e4f41a75fbe5319e48383"
      ],
      "layout": "IPY_MODEL_48d161e77c1d4e40beecb0f263d54763"
     }
    },
    "1abbc6518ef7445bbff2559b1a7cca2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32b5a64c38474eb3b05eb1cde1994927": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3494b43c49c14b009a7e03ae6798f818": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "35d9922f836f4fd2b0cdfe447af09bca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3867dfbe772141b58ff8728a201f76dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81d4444cb8414b56990835b2ed99677e",
      "placeholder": "â",
      "style": "IPY_MODEL_7802e2b380da47a2979a6e4ec29da921",
      "value": "â60000/0â[00:00&lt;00:00,â83765.10âexamples/s]"
     }
    },
    "3944d43006094c3ead030b6ffe505815": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3cb558d03c524075abfc67a154ec4f0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3494b43c49c14b009a7e03ae6798f818",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5817af58e0c443f5bd226645b0157e78",
      "value": 1
     }
    },
    "474e9082a92c45d295fcc8faa09fe4af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5eda53205b14617b72c40919dc33d65",
      "placeholder": "â",
      "style": "IPY_MODEL_d759c3f33a6342ca838aa7935997ceea",
      "value": "100%"
     }
    },
    "48d161e77c1d4e40beecb0f263d54763": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b48e00446464125ad5f139ba9e68c7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e04bb3c232a469bac1001aa5ac8a561": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a537b8225fb743db9c3054959b2c2234",
      "placeholder": "â",
      "style": "IPY_MODEL_d8dfe23f85134fd6b02e07e2fc2aa25d",
      "value": "100%"
     }
    },
    "508a0fc496f445dbb37a177c3b2ff4bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55f4b904d9404e8cbf9e1a5c8c934436": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5817af58e0c443f5bd226645b0157e78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5980b89f1166499ba6185311002463f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fd05eefe82d4e63b9fda9a6124e5153",
      "placeholder": "â",
      "style": "IPY_MODEL_0676ba35b6194a078f53b30ad05f4991",
      "value": "â20/20â[05:48&lt;00:00,â17.36s/it,âepoch=20/20,âloss=0.0452,âval_loss=0.0452]"
     }
    },
    "5ec243527f6540009166f1147122d4f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "638d985fe9da4d2aa7d34447049be1f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6e0683251574417793ac741546fcafef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7802e2b380da47a2979a6e4ec29da921": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d6cf4cf803745e4b959b89eb3244256": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d7ef7296f0a41049bf1c81a65987a4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4ea9a4bb14e4df19d105c81ce6f2e6d",
      "placeholder": "â",
      "style": "IPY_MODEL_80dabca945a246a288c9556aeb303da4",
      "value": "â50/50â[11:45&lt;00:00,â14.07s/it,âepoch=50/50,âloss=0.0301,âval_loss=0.0300]"
     }
    },
    "7e7868cebe47479e82ea30763c86edca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec4522af12ec4cdf80d3c29d425ac273",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0da5b7e0ce25441cba444e632ef62340",
      "value": 1
     }
    },
    "7ed5c6c6f5c749b2849b203da467f8e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ee5ddf3b47b46feaa3eeb372f489c98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "80dabca945a246a288c9556aeb303da4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "81d4444cb8414b56990835b2ed99677e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8552e02fdd1b480bb09e64af54fb2829": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03b115e9fd9e4f6bacee75bf1e91735f",
      "placeholder": "â",
      "style": "IPY_MODEL_3944d43006094c3ead030b6ffe505815",
      "value": "Generatingâtrainâsplit:â"
     }
    },
    "85c0efd7ac52436a83aeac87dc34f960": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8bc1b3f02e3543989bacf0aa882589e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f47b7e2b591402eba7cafdfd404a01c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8f5f98a52e824aeb921b99cb6f3527bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d6cf4cf803745e4b959b89eb3244256",
      "placeholder": "â",
      "style": "IPY_MODEL_deeec5762ea54fbcb136e47696434be9",
      "value": "100%"
     }
    },
    "8f9a49a562f341bbbb71a27a06a6ad1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4e04bb3c232a469bac1001aa5ac8a561",
       "IPY_MODEL_db4cb3d76be84a52830ddb970ab3fe89",
       "IPY_MODEL_5980b89f1166499ba6185311002463f2"
      ],
      "layout": "IPY_MODEL_1179f40122794b168dc0665d77b3bd15"
     }
    },
    "94baa119ed9845928991ca3440029131": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94db787b4fe04f3a9aafccabc73e9f31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "983dbd1084cd43c1835a398d9a51d22e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_474e9082a92c45d295fcc8faa09fe4af",
       "IPY_MODEL_f793438494ab4d27acb3c57705c7bffd",
       "IPY_MODEL_7d7ef7296f0a41049bf1c81a65987a4c"
      ],
      "layout": "IPY_MODEL_32b5a64c38474eb3b05eb1cde1994927"
     }
    },
    "9936fe021fa74ea2a3429cacbfb252cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c09c1eba258404f870ba0895b638570": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a537b8225fb743db9c3054959b2c2234": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b321467d620a4a908cd906a6c6d28a17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55f4b904d9404e8cbf9e1a5c8c934436",
      "placeholder": "â",
      "style": "IPY_MODEL_7ee5ddf3b47b46feaa3eeb372f489c98",
      "value": "100%"
     }
    },
    "b58249e4ec5a467fb40c70917b0f2eaa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8552e02fdd1b480bb09e64af54fb2829",
       "IPY_MODEL_3cb558d03c524075abfc67a154ec4f0d",
       "IPY_MODEL_3867dfbe772141b58ff8728a201f76dc"
      ],
      "layout": "IPY_MODEL_7ed5c6c6f5c749b2849b203da467f8e2"
     }
    },
    "b5eda53205b14617b72c40919dc33d65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4ea9a4bb14e4df19d105c81ce6f2e6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4f4e17e87904108b4442cdb36443429": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35d9922f836f4fd2b0cdfe447af09bca",
      "max": 15,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_638d985fe9da4d2aa7d34447049be1f6",
      "value": 15
     }
    },
    "c705c3f1b21e4f41a75fbe5319e48383": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fedb0e6df744ad7b32dd5f14a493b61",
      "placeholder": "â",
      "style": "IPY_MODEL_9c09c1eba258404f870ba0895b638570",
      "value": "â70/70â[21:09&lt;00:00,â18.25s/it,âepoch=70/70,âloss=0.0843,âval_loss=0.0841]"
     }
    },
    "cb8f00dac91e47b3b81084de7ac52e62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d607db3bc256454092cb1eb024901c20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94db787b4fe04f3a9aafccabc73e9f31",
      "placeholder": "â",
      "style": "IPY_MODEL_9936fe021fa74ea2a3429cacbfb252cb",
      "value": "â10000/0â[00:00&lt;00:00,â91384.75âexamples/s]"
     }
    },
    "d759c3f33a6342ca838aa7935997ceea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8dfe23f85134fd6b02e07e2fc2aa25d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db4cb3d76be84a52830ddb970ab3fe89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e0683251574417793ac741546fcafef",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5ec243527f6540009166f1147122d4f1",
      "value": 20
     }
    },
    "deeec5762ea54fbcb136e47696434be9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec4522af12ec4cdf80d3c29d425ac273": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "f03adde2b61040daa05a9a981512af51": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3e816556c95470794f40759a8d8c58e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94baa119ed9845928991ca3440029131",
      "placeholder": "â",
      "style": "IPY_MODEL_85c0efd7ac52436a83aeac87dc34f960",
      "value": "â15/15â[03:57&lt;00:00,â15.77s/it,âepoch=15/15,âloss=0.0342,âval_loss=0.0339]"
     }
    },
    "f793438494ab4d27acb3c57705c7bffd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1abbc6518ef7445bbff2559b1a7cca2f",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb8f00dac91e47b3b81084de7ac52e62",
      "value": 50
     }
    },
    "fae564eac5eb4d95a5fda6b3e6b083de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b48e00446464125ad5f139ba9e68c7b",
      "max": 70,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8f47b7e2b591402eba7cafdfd404a01c",
      "value": 70
     }
    },
    "fb197b7ef6d6484e80a4f77f6f03b1e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
