{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smjlo1x48N3t"
   },
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6H2W9QmdP0U"
   },
   "source": [
    "<p>\n",
    "CAS on Advanced Machine Learning <br>\n",
    "Data Science Lab, University of Bern, 2024<br>\n",
    "Prepared by Dr. Mykhailo Vladymyrov.\n",
    "\n",
    "</p>\n",
    "\n",
    "This work is licensed under a <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZiWkvrVdiiA"
   },
   "source": [
    "# Libs and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GynY3qcttQPu"
   },
   "outputs": [],
   "source": [
    "# on colab:\n",
    "# !pip install einops\n",
    "# !pip install mlflow\n",
    "# !pip install optuna\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TDxRnvL7x3Q"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjo8ffOptAcP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set env var to allow duplicated lib\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "This code snippet is doing the following:\n",
    "\n",
    "#import os - Imports Python's built-in operating system module, which provides functions for interacting with the operating system.\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='TRUE' - Sets an environment variable called 'KMP_DUPLICATE_LIB_OK' to 'TRUE'.\n",
    "\n",
    "#This specific environment variable is related to Intel's Math Kernel Library (MKL), which PyTorch often uses for performance optimization on Intel CPUs. The setting is addressing a known issue where the Intel OpenMP runtime library might be loaded multiple times, which can cause warnings or errors on some systems (particularly macOS).\n",
    "#By setting this to 'TRUE', you're essentially telling the system to ignore the duplicate library loading issue. This is a common workaround when using PyTorch on macOS to prevent warnings about duplicate libraries being loaded.\n",
    "#This line doesn't affect the functionality of your model, but rather helps avoid environment-related warnings or errors during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRCK8pzztAcQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import einops as eo\n",
    "import pathlib as pl\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import collections  as mc\n",
    "from matplotlib import animation\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from time import time as timer\n",
    "#import umap\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Audio\n",
    "import IPython\n",
    "\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "is_colab = 'google.colab' in sys.modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlSvdkTItAcR"
   },
   "outputs": [],
   "source": [
    "# get mean and std of an array with numpy:\n",
    "def get_mean_std(x):\n",
    "    x_mean = np.mean(x)\n",
    "    x_std = np.std(x)\n",
    "    return x_mean, x_std\n",
    "\n",
    "# get min and max of an array with numpy:\n",
    "def get_min_max(x):\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    return x_min, x_max\n",
    "\n",
    "def is_iterable(obj):\n",
    "    try:\n",
    "        iter(obj)\n",
    "    except Exception:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "#This function checks if an object is iterable (can be looped over).\n",
    "#It uses a try-except block to attempt to call iter(obj), which will succeed only if obj is iterable.\n",
    "#If calling iter(obj) raises any exception, the function returns False.\n",
    "#If no exception occurs, the function returns True.\n",
    "\n",
    "def type_len(obj):\n",
    "    t = type(obj)\n",
    "    if is_iterable(obj):\n",
    "        sfx = f', shape: {obj.shape}' if t == np.ndarray else ''\n",
    "        print(f'type: {t}, len: {len(obj)}{sfx}')\n",
    "    else:\n",
    "        print(f'type: {t}, len: {len(obj)}')\n",
    "\n",
    "#This is a utility function for debugging that prints information about an object.\n",
    "#t = type(obj) - Gets the type of the provided object.\n",
    "#It checks if the object is iterable using the is_iterable function defined earlier.\n",
    "#If the object is iterable:\n",
    "#\n",
    "#It checks if the object is a NumPy array (t == np.ndarray).\n",
    "#If it's a NumPy array, it adds shape information to the output string.\n",
    "#It prints the type and length of the object, along with shape information if applicable.\n",
    "#\n",
    "#\n",
    "#If the object is not iterable, it still attempts to print the type and length (though this might raise an error if len() isn't applicable to the object).\n",
    "#\n",
    "#Note: There seems to be an issue with the type_len function - it tries to call len() on non-iterable objects in the else clause, \n",
    "#which would typically cause an error. This might be a bug in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N78bzRywtAcc"
   },
   "outputs": [],
   "source": [
    "def to_np_showable(pt_img):\n",
    "  np_im = pt_img.detach().cpu().numpy()\n",
    "  if len(np_im.shape) == 4:\n",
    "    np_im = np_im[0]\n",
    "\n",
    "  if np_im.shape[0] > 3:\n",
    "    np_im = np_im[-3:]\n",
    "\n",
    "  return (eo.rearrange(np_im, 'c h w -> h w c')/2+.5).clip(0., 1.)\n",
    "\n",
    "#This function converts a PyTorch tensor image to a NumPy array suitable for visualization.\n",
    "#pt_img.detach().cpu().numpy() - Detaches the tensor from the computation graph, moves it to CPU if it's on GPU, and converts it to a NumPy array.\n",
    "#if len(np_im.shape) == 4: - Checks if the image has a batch dimension (shape: [batch, channels, height, width]).\n",
    "#np_im = np_im[0] - If there's a batch dimension, takes only the first image in the batch.\n",
    "#if np_im.shape[0] > 3: - Checks if there are more than 3 channels.\n",
    "#np_im = np_im[-3:] - If there are more than 3 channels, keeps only the last 3 channels (useful for handling multi-channel data).\n",
    "#eo.rearrange(np_im, 'c h w -> h w c') - Uses the einops library to rearrange the tensor from PyTorch's [channels, height, width] format to matplotlib's [height, width, channels] format.\n",
    "#/2+.5 - Applies normalization assuming the image data is in the range [-1, 1], converting it to [0, 1].\n",
    "#.clip(0., 1.) - Ensures all values are within the [0, 1] range, clamping any values outside this range.\n",
    "\n",
    "def plot_im(im, is_torch=True):\n",
    "  plt.imshow(to_np_showable(im) if is_torch else im, cmap='gray')\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "#This function plots a single image.\n",
    "#is_torch=True - Default parameter indicating whether the input is a PyTorch tensor.\n",
    "#to_np_showable(im) if is_torch else im - Converts the image to a NumPy array if it's a PyTorch tensor, otherwise uses it directly.\n",
    "#plt.imshow(..., cmap='gray') - Displays the image using matplotlib with a grayscale colormap.\n",
    "#plt.show() - Renders the plot.\n",
    "#plt.close() - Closes the figure to free up memory.\n",
    "\n",
    "def plot_im_samples(ds, n=5, is_torch=False):\n",
    "  fig, axs = plt.subplots(1, n, figsize=(16, n))\n",
    "  for i, image in enumerate(ds[:n]):\n",
    "      axs[i].imshow(to_np_showable(image) if is_torch else image, cmap='gray')\n",
    "      axs[i].set_axis_off()\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "#This function plots multiple images from a dataset in a row.\n",
    "#ds - The dataset or collection of images to sample from.\n",
    "#n=5 - Default number of images to display.\n",
    "#is_torch=False - Default parameter indicating whether the inputs are PyTorch tensors.\n",
    "#plt.subplots(1, n, figsize=(16, n)) - Creates a figure with a single row of n subplots, with a width of 16 inches and height of n inches.\n",
    "#The loop iterates through the first n images in the dataset:\n",
    "#\n",
    "#axs[i].imshow(...) - Displays each image in its corresponding subplot.\n",
    "#axs[i].set_axis_off() - Removes axis labels and ticks for cleaner visualization.\n",
    "#\n",
    "#\n",
    "#plt.show() - Renders the entire plot with all images.\n",
    "#plt.close() - Closes the figure to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyC2UKsetAce"
   },
   "outputs": [],
   "source": [
    "# merging 2d matrix of images in 1 image\n",
    "def mosaic(mtr_of_ims):\n",
    "  ny = len(mtr_of_ims)\n",
    "  assert(ny != 0)\n",
    "  #Gets the number of rows in the matrix and asserts that it's not empty.\n",
    "\n",
    "  nx = len(mtr_of_ims[0])\n",
    "  assert(nx != 0)\n",
    "  #Gets the number of columns in the first row and asserts that it's not empty.\n",
    "\n",
    "  im_sh = mtr_of_ims[0][0].shape\n",
    "\n",
    "  assert (2 <= len(im_sh) <= 3)\n",
    "  #Gets the shape of the first image in the matrix.\n",
    "  #Verifies that the image is either 2D (grayscale) or 3D (with channels).\n",
    "    \n",
    "  multichannel = len(im_sh) == 3\n",
    "\n",
    "  if multichannel:\n",
    "    h, w, c = im_sh\n",
    "  else:\n",
    "    h, w = im_sh\n",
    "  #Determines if the images have multiple channels.\n",
    "  #If multichannel, unpacks height, width, and channels. Otherwise, just height and width.\n",
    "\n",
    "  h_c = h * ny + 1 * (ny-1)\n",
    "  w_c = w * nx + 1 * (nx-1)\n",
    "  #Calculates the total height and width of the canvas.\n",
    "  #Adds 1 pixel spacing between images (both horizontally and vertically).\n",
    "\n",
    "  canv_sh = (h_c, w_c, c) if multichannel else (h_c, w_c)\n",
    "  canvas = np.ones(shape=canv_sh, dtype=np.float32)*0.5\n",
    "  #Defines the shape of the canvas based on whether images are multichannel.\n",
    "  #Creates a canvas filled with gray (0.5) values, assuming image values are in [0,1] range.\n",
    "\n",
    "  for iy, row in enumerate(mtr_of_ims):\n",
    "    y_ofs = iy * (h + 1)\n",
    "    #Loops through each row of images.\n",
    "    #Calculates the vertical offset for the current row.\n",
    "    for ix, im in enumerate(row):\n",
    "      x_ofs = ix * (w + 1)\n",
    "      #Loops through each image in the current row.\n",
    "      #Calculates the horizontal offset for the current image.\n",
    "      canvas[y_ofs:y_ofs + h, x_ofs:x_ofs + w] = im\n",
    "      #Copies the current image to the appropriate position in the canvas.\n",
    "      #This uses NumPy's array slicing to place the image at the correct location.\n",
    "  return canvas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "FHjhiIzRtAcf"
   },
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')  # use first available GPU\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se 'mps' è disponibile su Apple Silicon\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "\n",
    " \n",
    "\n",
    "# Conferma del dispositivo selezionato\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5NaXhkctAcg",
    "outputId": "55e8045c-60a2-4dec-d19f-37d7f0fb9619"
   },
   "source": [
    "device = get_device()\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux0WVvO6diiD"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3xcUuod8bUt"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMWqy2uMffPA"
   },
   "source": [
    "Lets start with a simple, well understood mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSU2wVsXtAcj"
   },
   "outputs": [],
   "source": [
    "NOISE_RATE = 0.1\n",
    "N_SAMPLE = 32\n",
    "N_VIS_SAMPLE = 2\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSPzUDchtAck"
   },
   "outputs": [],
   "source": [
    "def collate_ae_dataset(samples):\n",
    "    \"\"\"\n",
    "    The function collates sampels into a batch, and creates noisy samples if DENOISING is True\n",
    "    for the denoising autoencoder.\n",
    "    \"\"\"\n",
    "    xs = [s[0] for s in samples]\n",
    "    ys = [s[1] for s in samples]\n",
    "    #Extracts the first element (input data) from each sample into list xs.\n",
    "    #Extracts the second element (labels or targets) from each sample into list ys.\n",
    "    #This assumes each sample is a tuple or list with at least two elements.\n",
    "    \n",
    "    xs = torch.stack(xs)\n",
    "    ys = torch.concat(ys)\n",
    "    #torch.stack(xs) combines the list of input tensors into a single \n",
    "    #tensor along a new dimension (creating a batch dimension).\n",
    "    #torch.concat(ys) concatenates the label tensors along \n",
    "    #the existing first dimension. This suggests the labels might have \n",
    "    #variable lengths or already include a batch-like dimension.\n",
    "\n",
    "    add_noise = NOISE_RATE > 0.\n",
    "    #Checks if noise should be added based on a global variable NOISE_RATE.\n",
    "    #If NOISE_RATE is greater than 0, noise will be added to the inputs.\n",
    "    \n",
    "    if add_noise:\n",
    "      sh = xs.shape\n",
    "      noise_mask = torch.bernoulli(torch.full(sh, NOISE_RATE))  # 0 (keep) or 1 (replace with noise)\n",
    "      #Gets the shape of the input tensor batch.\n",
    "      #Creates a binary mask using Bernoulli sampling, where each element has NOISE_RATE probability of being 1 \n",
    "      #(indicating where noise will be applied) and 1-NOISE_RATE probability of being 0.\n",
    "            \n",
    "      sp_noise = torch.bernoulli(torch.full(sh, 0.5))-0.5  # -1 or 1\n",
    "      #Generates the actual noise values as either -0.5 or 0.5.\n",
    "      #First creates a tensor of the same shape filled with 0.5, then applies Bernoulli \n",
    "      #sampling to get 0s or 1s.\n",
    "      #Subtracts 0.5 to convert to -0.5 or 0.5 (this creates salt and pepper noise).\n",
    "        \n",
    "      xns = xs * (1-noise_mask) + sp_noise * noise_mask\n",
    "      #Creates the noisy input xns by:\n",
    "          #Keeping original values where the mask is 0: xs * (1-noise_mask)\n",
    "          #Adding noise values where the mask is 1: sp_noise * noise_mask\n",
    "          #The result is a tensor where some values are preserved \n",
    "          #from the original input and others are replaced with noise.\n",
    "      \n",
    "      # sp = sp_noise\n",
    "    else:\n",
    "       xns = xs\n",
    "    #If no noise is to be added, the noisy input is the same as the original input.\n",
    "\n",
    "    return xns.to(device), xs.to(device), ys.to(device)\n",
    "    #Returns three tensors, all moved to the specified device (likely GPU):\n",
    "\n",
    "    #xns: The inputs with noise added (or original inputs if no noise)\n",
    "    #xs: The original clean inputs\n",
    "    #ys: The labels or targets\n",
    "    #\n",
    "    #\n",
    "    #This return structure is typical for denoising autoencoders, where you need \n",
    "    #both the noisy input (fed to the encoder) and the clean target \n",
    "    #(used to compute the reconstruction loss).\n",
    "    #\n",
    "    #This function is specifically designed for training denoising autoencoders, \n",
    "    #where the model learns to remove noise from corrupted inputs by trying \n",
    "    #to reconstruct the original clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ilgX4HUUtAcl",
    "outputId": "ddb99624-89f2-4f60-e14e-49c35d6f2891"
   },
   "outputs": [],
   "source": [
    "m, s = 0.5, 1.\n",
    "# m, s = 0.5, 0.5\n",
    "#m, s = 0., 1.\n",
    "\n",
    "#Defines normalization parameters for the images: mean (m) and standard deviation (s).\n",
    "#The active values are m=0.5 and s=1.0.\n",
    "#The commented lines show alternative normalization parameters that were tried.\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Pad(2), # to make images 32x32\n",
    "    transforms.Normalize((m,), (s,))\n",
    "])\n",
    "#Creates a transformation pipeline for the images:\n",
    "   #\n",
    "   #transforms.ToTensor(): Converts PIL images to PyTorch tensors and scales pixel values from [0, 255] to [0, 1].\n",
    "   #The commented line would add padding to make images 32×32 (original MNIST is 28×28).\n",
    "   #transforms.Normalize((m,), (s,)): Normalizes the tensor with the specified mean and standard deviation, transforming values to (pixel - m) / s.\n",
    "   #With m=0.5, s=1.0, this shifts the [0, 1] pixel values to [-0.5, 0.5].\n",
    "\n",
    "lable_transform = transforms.Compose([lambda x:torch.LongTensor([x])])\n",
    "#Creates a transformation for the labels:\n",
    "   #\n",
    "   #Takes a numeric label and converts it to a PyTorch tensor of type Long (integer).\n",
    "   #Wraps it in a list to create a 1D tensor with a single value.\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform, target_transform=lable_transform)\n",
    "valid_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform, target_transform=lable_transform)\n",
    "#Creates two MNIST dataset objects:\n",
    "    #\n",
    "    #train_dataset: Uses the training split of MNIST (60,000 images).\n",
    "    #valid_dataset: Uses the test split of MNIST (10,000 images) for validation.\n",
    "    #Both will download the dataset if not already present in './data'.\n",
    "    #Both apply the previously defined transforms to images and labels.\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_ae_dataset, drop_last=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_ae_dataset, drop_last=True)\n",
    "#Creates DataLoader objects for efficient batch processing:\n",
    "    #\n",
    "    #train_loader: For training data, with shuffling enabled to randomize the order.\n",
    "    #valid_loader: For validation data, without shuffling to maintain order.\n",
    "    #Both use the custom collate_ae_dataset function we analyzed earlier, which adds noise to inputs for denoising autoencoder training.\n",
    "    #drop_last=True: Discards the last batch if it's smaller than the specified batch size.\n",
    "    #BATCH_SIZE is a global variable defined elsewhere in the code.\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    #This setup prepares MNIST data specifically for training a denoising autoencoder, with the ability to add controlled noise to the input images while keeping the original clean images as targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgnSKMqodiiE"
   },
   "outputs": [],
   "source": [
    "# # given a AE model `model`\n",
    "# for img, label in valid_dataset:\n",
    "#     reconstruction = model(img)\n",
    "#     loss_value = loss(img, reconstruction).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxNLMg4pdiiF",
    "outputId": "c1c5d701-b53d-451c-da5e-506e0619c14d"
   },
   "outputs": [],
   "source": [
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "gtGMw5KXdiiF",
    "outputId": "539c1627-750d-40de-858b-e36a511b5cd7"
   },
   "outputs": [],
   "source": [
    "#This code loads and examines a single sample from the validation dataset:\n",
    "for sample in valid_dataset:\n",
    "    img, label = sample\n",
    "    print(type_len(img))\n",
    "    print(type_len(label))\n",
    "    print(img.shape, label.shape)\n",
    "    plt.hist(img.flatten(), bins=100)\n",
    "    break\n",
    "#Here's what each line does:\n",
    "    #\n",
    "    #for sample in valid_dataset: - Starts iterating through the validation dataset.\n",
    "    #img, label = sample - Unpacks the first sample into its image and label components. \n",
    "    #Each sample from the dataset is a tuple of (image, label).\n",
    "    #print(type_len(img)) - Calls the previously defined type_len function on the image tensor. \n",
    "    #This will print information about the type and shape of the image.\n",
    "    #print(type_len(label)) - Similarly, prints type and shape information for the label.\n",
    "    #print(img.shape, label.shape) - Directly prints the shapes of both the image and label tensors.\n",
    "    #plt.hist(img.flatten(), bins=100) - Creates a histogram of all pixel values in the image:\n",
    "    #\n",
    "    #img.flatten() converts the multi-dimensional tensor into a 1D array\n",
    "    #bins=100 divides the range of values into 100 bins for the histogram\n",
    "    #This visualizes the distribution of pixel values after the transformations (ToTensor and Normalize)\n",
    "    #\n",
    "    #\n",
    "    #break - Exits the loop after processing just the first sample, so only one image is examined.\n",
    "    #\n",
    "    #The histogram will show the distribution of normalized pixel values, which should be centered around 0 (since the normalization used m=0.5 and s=1.0, shifting the original [0,1] values to approximately [-0.5, 0.5]).\n",
    "    #This code snippet is useful for understanding the exact format and value ranges of the data after all transformations have been applied, which is important for debugging and confirming that preprocessing is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "hQ5SpeMddiiF",
    "outputId": "750bacf7-e823-4f5f-dc28-aeafcc40b121"
   },
   "outputs": [],
   "source": [
    "#This line of code creates a histogram of the raw MNIST validation dataset pixel values:\n",
    "\n",
    "plt.hist(valid_dataset.data.numpy().flatten(), bins=100);\n",
    "\n",
    "#Let's break it down:\n",
    "#\n",
    "#valid_dataset.data - Accesses the raw image data from the MNIST validation dataset. This is the original pixel data before any transformations are applied.\n",
    "#.numpy() - Converts the PyTorch tensor to a NumPy array, since the plotting functions in matplotlib work with NumPy arrays.\n",
    "#.flatten() - Converts the multi-dimensional array (which has shape [10000, 28, 28] for the 10,000 MNIST test images, each 28×28 pixels) into a 1D array with all pixel values.\n",
    "#plt.hist(..., bins=100) - Creates a histogram with 100 bins, showing the distribution of all pixel values.\n",
    "#The semicolon at the end suppresses any additional output that might be generated.\n",
    "#\n",
    "#This histogram will show the original, untransformed distribution of pixel values in the MNIST dataset. Since MNIST consists of grayscale images with pixel values from 0 to 255, you'll see a distribution heavily skewed toward 0 (black), with a much smaller number of pixels having higher values (gray to white). This is typical for MNIST, which has black backgrounds and white digit strokes.\n",
    "#This is useful for comparing against the transformed data to visualize exactly how your normalization is affecting the pixel value distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of PyTorch dataset classes: \n",
    "### Me:\n",
    "Let me ask a question about the torchvision.dataset: Looping through valid_dataset yields the transformed data whereas valid_dataset.data accesses the original untransformed data, if my understanding is correct. Is this correct? Is this a general property of the torchvision dataset type (am I correct in calling this object torchvision data set?)?Edit\n",
    "\n",
    "### Claude: \n",
    "Yes, your understanding is correct. When you loop through a PyTorch dataset object like valid_dataset, you get the transformed data (after all transforms have been applied), while accessing valid_dataset.data gives you the original, untransformed data.\n",
    "This is indeed a general property of torchvision datasets. The datasets.MNIST class (and other dataset classes in torchvision.datasets) are designed this way deliberately:\n",
    "\n",
    "valid_dataset.data - Contains the raw, untransformed tensor data\n",
    "valid_dataset.targets - Contains the raw, untransformed labels\n",
    "When you iterate through the dataset or use valid_dataset[index], you get the data after both the transform and target_transform functions have been applied\n",
    "\n",
    "This design allows you to:\n",
    "\n",
    "Apply transformations efficiently (only when data is actually accessed)\n",
    "Access the original data when needed (for visualization or debugging)\n",
    "Maintain a single copy of the dataset in memory\n",
    "\n",
    "This separation between raw data and transformed data is a general pattern in PyTorch's dataset classes, not just in torchvision. Custom datasets typically follow the same pattern, where the __getitem__ method applies transformations on-the-fly, but the raw data is still accessible through class attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3308k_x0diiF",
    "outputId": "2d34ae8e-e909-4071-e0f5-8c2b1c002818"
   },
   "outputs": [],
   "source": [
    "valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSjk2chCdiiG",
    "outputId": "874b07e0-b062-4092-8137-9feb55ec893c"
   },
   "outputs": [],
   "source": [
    "for sample in valid_loader:\n",
    "    #- Begins iteration through the validation data loader, which provides batches of data.\n",
    "\n",
    "    noisy_img, img, label = sample\n",
    "    #Unpacks the first batch from the data loader into three components:\n",
    "        #noisy_img: The input images with noise added (for denoising autoencoder training)\n",
    "        #img: The original clean images (targets for reconstruction)\n",
    "        #label: The class labels for the images\n",
    "    print(type_len(noisy_img))\n",
    "    print(type_len(img))\n",
    "    print(type_len(label))\n",
    "    \n",
    "    print(noisy_img.shape, img.shape, label.shape)\n",
    "    #Directly prints the shapes of all three tensors in the batch.\n",
    "    \n",
    "    #plt.hist(img.flatten(), bins=100)\n",
    "    #A commented-out line that would create a histogram of all pixel values in the clean image batch if uncommented.\n",
    "    \n",
    "    break\n",
    "    #Exits the loop after processing just the first batch.\n",
    "\n",
    "\n",
    "#This code is useful for confirming that the data loader and the custom collate \n",
    "    #function are working as expected, providing the three tensors in the expected shapes. \n",
    "    #It's particularly important to verify that the noisy and clean images have the same shape, \n",
    "    #and that the batch size matches what was specified in the DataLoader setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Me: \n",
    "Thanks for your explanations. Could you also explain properties and behaviour of the valid_loader object? It is the output of the torch.utils.data.DataLoader() function applied to the valid_dataset object if I understand correctly.\n",
    "\n",
    "\n",
    "### Claude: \n",
    "You're welcome! Let me explain the properties and behavior of the valid_loader object.\n",
    "Yes, valid_loader is indeed the output of the torch.utils.data.DataLoader() function applied to the valid_dataset object. The DataLoader is a crucial PyTorch utility that wraps a dataset and provides several important features:\n",
    "Key Properties and Behaviors of valid_loader:\n",
    "\n",
    "#### Batching:\n",
    "\n",
    "Takes individual samples from the dataset and combines them into batches\n",
    "The batch size was defined by the batch_size=BATCH_SIZE parameter\n",
    "Each iteration returns a batch instead of a single sample\n",
    "\n",
    "\n",
    "#### Custom Collation:\n",
    "\n",
    "Uses the collate_fn=collate_ae_dataset parameter to customize how samples are combined\n",
    "In your case, this is where noise is added to create the noisy images for denoising autoencoder training\n",
    "This function transforms the standard (image, label) pairs into (noisy_image, clean_image, label) triplets\n",
    "\n",
    "\n",
    "#### No Shuffling:\n",
    "\n",
    "shuffle=False means samples are processed in sequential order\n",
    "This is typical for validation sets where randomization isn't needed\n",
    "\n",
    "\n",
    "#### Dropping Incomplete Batches:\n",
    "\n",
    "drop_last=True means that if the number of samples isn't divisible by the batch size, the last incomplete batch is discarded\n",
    "This ensures all batches have consistent size, which can be important for some operations\n",
    "\n",
    "\n",
    "#### Iteration Interface:\n",
    "\n",
    "Can be iterated through using a for loop: for batch in valid_loader:\n",
    "Each iteration yields one batch of data\n",
    "For validation data, you typically iterate through the entire loader once per epoch\n",
    "\n",
    "\n",
    "#### Memory Efficiency:\n",
    "\n",
    "Loads data in batches on-demand rather than loading the entire dataset at once\n",
    "Applies transformations on-the-fly\n",
    "\n",
    "\n",
    "#### Multi-processing Support (if enabled):\n",
    "\n",
    "Can load data in parallel using multiple worker processes\n",
    "This wasn't explicitly set in your code, so it's using the default (single process)\n",
    "\n",
    "\n",
    "#### Length Property:\n",
    "\n",
    "len(valid_loader) would return the number of batches\n",
    "With drop_last=True, this equals floor(len(valid_dataset) / BATCH_SIZE)\n",
    "\n",
    "\n",
    "#### Device Handling:\n",
    "\n",
    "The collate function in your code moves data to the specified device (likely GPU)\n",
    "This saves you from explicitly moving data to the right device in your training loop\n",
    "\n",
    "\n",
    "\n",
    "When you iterate through valid_loader, you get batches in the format determined by your custom collate function. In your case, each batch contains three tensors: noisy images, clean images, and labels, all already moved to the target device and ready for model processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1sZyi9IPtAcr",
    "outputId": "d0103f9e-0908-4fff-97b2-195059005aa7"
   },
   "outputs": [],
   "source": [
    "for s in train_loader:\n",
    "  xns, xs, ys = s\n",
    "  print(xns.shape, xs.shape, ys.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "PIPvKCH9tAcs",
    "outputId": "19412e44-73ef-42ea-b03b-31a0a614db6e"
   },
   "outputs": [],
   "source": [
    "plot_im_samples(xns, is_torch=True)\n",
    "plot_im_samples(xs, is_torch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mMqQFO_tAcs"
   },
   "outputs": [],
   "source": [
    "# fill array of all preprocessed training samples, converted to numpy:\n",
    "train_images = []\n",
    "# - Initializes an empty list to store batches of images.\n",
    "\n",
    "for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
    "# Iterates through all batches in the training data loader:\n",
    "# batch_idx is the index of the current batch\n",
    "# noisy_data, data, target unpacks each batch into its three components\n",
    "#enumerate() provides both the batch index and the batch contents\n",
    "    \n",
    "    train_images.append(data.detach().cpu().numpy())\n",
    "    # For each batch:\n",
    "        # data.detach() removes the tensor from the computation graph (no gradients needed)\n",
    "        # .cpu() moves the tensor to CPU memory if it was on another device like GPU\n",
    "        # .numpy() converts the PyTorch tensor to a NumPy array\n",
    "        # The result is appended to the train_images list\n",
    "\n",
    "train_images = np.concatenate(train_images, axis=0)\n",
    "# - Combines all batches into a single NumPy array:\n",
    "# axis=0 specifies to concatenate along the first dimension (the batch dimension)\n",
    "# The result is a single large array containing all training images\n",
    "\n",
    "\n",
    "# This process creates a complete NumPy array of all the training images, which could be useful for:\n",
    "# \n",
    "# Performing dataset-wide statistics or analysis\n",
    "# Visualizing the entire dataset distribution\n",
    "# Pre-computing values for normalization\n",
    "# Creating custom visualizations of the training data\n",
    "# \n",
    "# Note that this operation can be memory-intensive for large datasets, as it loads all images into memory at once, unlike the DataLoader which processes data in batches.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZmN0NguPtAc3",
    "outputId": "9ac0ded0-b276-4b86-95d5-fa33ddd7c80c"
   },
   "outputs": [],
   "source": [
    "print (\"train_images.shape = \", train_images.shape)\n",
    "print (\"train_images.dtype = \", train_images.dtype)\n",
    "print (\"train_images.mean/std() = \", get_mean_std(train_images))\n",
    "print (\"train_images.min/max() = \", get_min_max(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "uafPhG4EtAc4",
    "outputId": "2e8748f2-2f98-4a3c-e3ff-a89323514895"
   },
   "outputs": [],
   "source": [
    "# This line of code creates a histogram visualization of all pixel values in the training dataset:\n",
    "plt.hist(train_images.flatten(), bins=100, log=True);\n",
    "\n",
    "# Breaking it down:\n",
    "# \n",
    "# train_images.flatten() - Converts the multi-dimensional array of all training images (which likely has shape [num_samples, 1, 28, 28] for MNIST) into a one-dimensional array containing every pixel value from all images.\n",
    "# plt.hist(..., bins=100, log=True) - Creates a histogram with the following properties:\n",
    "# \n",
    "# bins=100 divides the range of pixel values into 100 equal-width bins\n",
    "# log=True uses a logarithmic scale for the y-axis (frequency count), which helps visualize the distribution when some bins have very high counts compared to others\n",
    "# \n",
    "# \n",
    "# The semicolon at the end suppresses any additional output that might be generated.\n",
    "# \n",
    "# This visualization shows the distribution of all pixel values across the entire training dataset after preprocessing (normalization). With the normalization parameters you used (m=0.5, s=1.0), the histogram will likely show a bimodal distribution:\n",
    "# \n",
    "# A large peak around -0.5 (corresponding to the black background pixels that were originally near 0)\n",
    "# A smaller peak around 0.5 (corresponding to the white digit strokes that were originally near 255)\n",
    "# \n",
    "# The logarithmic scale helps to see both the very common background pixel values and the less common digit pixel values in the same visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ta5guwIstAc4"
   },
   "outputs": [],
   "source": [
    "del train_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8kScAZTtAc5"
   },
   "source": [
    "We will also prepare a subsampled dataset from the validation set for the visualisation purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koccayybtAc5"
   },
   "outputs": [],
   "source": [
    "def get_samples(valid_loader):\n",
    "  # 1. get numpy array of all validation images:\n",
    "  val_images_noisy = []\n",
    "  val_images = []\n",
    "  val_labels = []\n",
    "\n",
    "  for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
    "      val_images_noisy.append(noisy_data.detach().cpu().numpy())\n",
    "      val_images.append(data.detach().cpu().numpy())\n",
    "      val_labels.append(target.detach().cpu().numpy())\n",
    "\n",
    "  val_images_noisy = np.concatenate(val_images_noisy, axis=0)\n",
    "  val_images = np.concatenate(val_images, axis=0)\n",
    "  val_labels = np.concatenate(val_labels, axis=0)\n",
    "\n",
    "  # 2. get numpy array of balanced validation samples for visualization:\n",
    "  sample_images_noisy = []\n",
    "  sample_images = []\n",
    "  sample_labels = []\n",
    "  single_el_idx = []  # indexes of single element per class for visualization\n",
    "\n",
    "  n_class = np.max(val_labels) + 1\n",
    "  # Determines the number of classes (for MNIST, this would be 10, representing digits 0-9).\n",
    "  for class_idx in range(n_class):\n",
    "    map_c = val_labels == class_idx\n",
    "\n",
    "    ims_c_noisy = val_images_noisy[map_c]\n",
    "    ims_c = val_images[map_c]\n",
    "    # For each class:\n",
    "       # Creates a boolean mask map_c identifying all samples of the current class.\n",
    "       # Extracts noisy and clean images for just this class.\n",
    "      \n",
    "\n",
    "    samples_idx = np.random.choice(len(ims_c), N_SAMPLE, replace=False)\n",
    "\n",
    "    ims_c_noisy_samples = ims_c_noisy[samples_idx]\n",
    "    ims_c_samples = ims_c[samples_idx]\n",
    "    # Randomly selects N_SAMPLE images from the current class.\n",
    "    # replace=False ensures no duplicates are selected.\n",
    "    # Extracts both noisy and clean versions of these sampled images.\n",
    "      \n",
    "\n",
    "    sample_images_noisy.append(ims_c_noisy_samples)\n",
    "    sample_images.append(ims_c_samples)\n",
    "\n",
    "    sample_labels.append([class_idx]*N_SAMPLE)\n",
    "\n",
    "    # Adds the sampled noisy images, clean images, and labels to their respective lists.\n",
    "    # Creates an array of N_SAMPLE repeated labels for this class.\n",
    "\n",
    "    start_idx = N_SAMPLE*class_idx\n",
    "    single_el_idx.extend([start_idx + i for i in range(min(N_VIS_SAMPLE, N_SAMPLE))])\n",
    "    # Calculates the indices for the first N_VIS_SAMPLE elements of this class in the final concatenated array.\n",
    "    # These indices will be used to extract a smaller subset for visualization.\n",
    "\n",
    "    \n",
    "  sample_images_noisy = np.concatenate(sample_images_noisy, axis=0)\n",
    "  sample_images = np.concatenate(sample_images, axis=0)\n",
    "  sample_labels = np.concatenate(sample_labels, axis=0)\n",
    "  single_el_idx = np.array(single_el_idx)\n",
    "  #Combines all class samples into single arrays.\n",
    "  #Converts the index list to a NumPy array.\n",
    "\n",
    "  samples = {\n",
    "      'images_noisy': sample_images_noisy,\n",
    "      'images': sample_images,\n",
    "      'labels': sample_labels,\n",
    "      'single_el_idx': single_el_idx\n",
    "\n",
    "  }\n",
    "  return samples\n",
    "# Creates and returns a dictionary with all collected samples.\n",
    "\n",
    "\n",
    "# This function ensures we have:\n",
    "# \n",
    "# A balanced number of samples for each class (equal representation)\n",
    "# Both noisy and clean versions of each image\n",
    "# A mapping between the noisy and clean versions\n",
    "# A subset of indices for visualization purposes\n",
    "# This is particularly useful for creating visualizations that show how the model behaves across different classes, or for comparing reconstruction quality across digits.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFJ_pQgrtAc6"
   },
   "outputs": [],
   "source": [
    "samples = get_samples(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYAEkBDKtAc6",
    "outputId": "e74399cd-c5ca-4242-dd23-f791eb6ef9f5"
   },
   "outputs": [],
   "source": [
    "samples['images'].shape, samples['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "id": "_F94TFGHtAc6",
    "outputId": "ce3b37b6-bdcf-483b-996a-4591020001cb"
   },
   "outputs": [],
   "source": [
    "single_el_idx = samples['single_el_idx']\n",
    "plot_im_samples(samples['images_noisy'][single_el_idx, 0], n=20, is_torch=False)\n",
    "plot_im_samples(samples['images'][single_el_idx, 0], n=20, is_torch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbR9emfD8d41"
   },
   "source": [
    "## Helper Autoencoder Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNM2dEZOh5EN"
   },
   "source": [
    "We will start from implementing an Autoencoder model base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGs_gV5GtAc8"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, code_size):\n",
    "        self.input_size = list(input_size)  # shape of data sample\n",
    "        self.flat_data_size = np.prod(self.input_size)\n",
    "        self.hidden_size = 128\n",
    "\n",
    "        self.code_size = code_size  # code size\n",
    "\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        #Creates an autoencoder neural network that inherits from PyTorch's nn.Module.\n",
    "        #Takes two parameters:\n",
    "        #\n",
    "        #input_size: The shape of input data (e.g., [1, 28, 28] for MNIST)\n",
    "        #code_size: The dimension of the encoded representation (bottleneck)\n",
    "        #\n",
    "        #\n",
    "        #Calculates the flattened input size by multiplying all dimensions.\n",
    "        #Sets an intermediate hidden layer size of 128 neurons.\n",
    "        #Calls the parent class initializer.\n",
    "\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(self.flat_data_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.hidden_size, self.code_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        #Defines the encoder network as a sequence of operations:\n",
    "            #\n",
    "            #Flattens the input (e.g., converts a 2D image to 1D)\n",
    "            #Linear layer mapping from input size to hidden size\n",
    "            #ReLU activation\n",
    "            #Linear layer mapping from hidden size to code size\n",
    "            #Sigmoid activation (constrains the encoded values to [0, 1])\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.code_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.hidden_size, self.flat_data_size),\n",
    "            nn.Tanh(),  # Think: why tanh?\n",
    "\n",
    "            nn.Unflatten(1, self.input_size),\n",
    "        )\n",
    "        #Defines the decoder network:\n",
    "            #Linear layer from code size to hidden size\n",
    "            #ReLU activation\n",
    "            #Linear layer from hidden size back to the flattened input size\n",
    "            #Tanh activation (outputs values in [-1, 1], matching the normalized input range)\n",
    "            #Unflattens the output back to the original input shape\n",
    "\n",
    "#Regarding \"why tanh?\": Tanh is used because the input images were normalized to approximately [-0.5, 0.5] \n",
    "    #range (using m=0.5, s=1.0). Tanh outputs values in the range [-1, 1], \n",
    "    #which after scaling by 1.1 in the decode method closely matches the input data range.\n",
    "\n",
    "    def forward(self, x, return_z=False):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return (decoded, encoded) if return_z else decoded\n",
    "    # The forward pass:\n",
    "        #Encodes the input\n",
    "        #Decodes the encoded representation\n",
    "        #If return_z=True, returns both the reconstruction and the encoded values\n",
    "        #Otherwise, just returns the reconstruction\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)*1.1\n",
    "# Helper methods to encode and decode separately\n",
    "# Note the multiplication by 1.1 in the decode method, \n",
    "    # which slightly amplifies the output range to better match the input data distribution\n",
    "\n",
    "        \n",
    "\n",
    "    def get_n_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    # Utility method to count the total number of trainable parameters in the model\n",
    "\n",
    "\n",
    "def eval_on_samples(ae_model, epoch, samples):\n",
    "    # this is called on end of each training epoch\n",
    "    xns = samples['images_noisy']\n",
    "    xns = torch.tensor(xns, dtype=torch.float32).to(device)\n",
    "    #labels = samples['labels']\n",
    "\n",
    "# Function to evaluate the autoencoder on sample data after each epoch\n",
    "# Takes the model, current epoch number, and samples dictionary\n",
    "# Extracts noisy images from the samples and converts them to a PyTorch tensor on the target device\n",
    "# The labels are extracted but commented out (not used)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yz = ae_model(xns, return_z=True)\n",
    "        yz = [el.detach().cpu().numpy() for el in yz]\n",
    "\n",
    "        y = yz[0]\n",
    "        z = yz[1:]\n",
    "    # Uses torch.no_grad() to disable gradient calculation (for efficiency during evaluation)\n",
    "    # Gets both reconstructions and encodings (i.e. latent space!) by calling the model with return_z=True\n",
    "    # Converts all outputs to NumPy arrays\n",
    "    # Separates the reconstructions y and encodings z\n",
    "\n",
    "    res = {'z': z, 'y': y, 'epoch': epoch}\n",
    "    return res\n",
    "\n",
    "# Creates and returns a dictionary containing:\n",
    "\n",
    "# z: The encoded representations\n",
    "# y: The reconstructed images\n",
    "# epoch: The current epoch number\n",
    "# \n",
    "\n",
    "# This evaluation function captures the model's performance at each epoch, allowing for tracking reconstruction quality and analyzing the learned representations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoITlVuntAc8"
   },
   "outputs": [],
   "source": [
    "def plot_hist(history, logscale=True):\n",
    "    \"\"\"\n",
    "    plot training loss\n",
    "    \"\"\"\n",
    "\n",
    "    loss = history['loss']\n",
    "    v_loss = history['val_loss']\n",
    "    epochs = history['epoch']\n",
    "\n",
    "    # This function visualizes training history (loss over epochs).\n",
    "    # Extracts training loss, validation loss, and epoch numbers from the history dictionary.\n",
    "\n",
    "    \n",
    "    plot = plt.semilogy if logscale else plt.plot\n",
    "    # Cleverly chooses between logarithmic scale (plt.semilogy) or linear scale (plt.plot) based on the logscale parameter.\n",
    "    # Default is logarithmic scale, which is often better for visualizing loss curves as they typically decrease exponentially.\n",
    "    \n",
    "    plot(epochs, loss, label='training');\n",
    "    plot(epochs, v_loss, label='validation');\n",
    "    # Plots both training and validation loss curves using the selected plotting function.\n",
    "    # Labels each curve for the legend.\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # Adds a legend, axis labels, displays the plot, and then closes the figure.\n",
    "\n",
    "\n",
    "\n",
    "def plot_samples(sample_history, samples, epoch_stride=5, fig_scale=1):\n",
    "    \"\"\"\n",
    "    Plots input, noisy samples (for DAE) and reconstruction.\n",
    "    Each `epoch_stride`-th epoch\n",
    "    \"\"\"\n",
    "    # This function visualizes sample reconstructions over training epochs.\n",
    "    # Shows how the model's reconstruction capability improves over time.\n",
    "\n",
    "    single_el_idx = samples['single_el_idx']\n",
    "    images_noisy = samples['images_noisy'][single_el_idx, 0]\n",
    "    images = samples['images'][single_el_idx, 0]\n",
    "    # Extracts indices for selected samples to visualize.\n",
    "    # Gets the noisy input images and the original clean images for these samples.\n",
    "    # The , 0 indexing suggests selecting the first channel of each image.\n",
    "\n",
    "    last_epoch = np.max(list(sample_history.keys()))\n",
    "    # Determines the last epoch number in the history data.\n",
    "\n",
    "    for epoch_idx, hist_el in sample_history.items():\n",
    "      if epoch_idx % epoch_stride != 0 and epoch_idx != last_epoch:\n",
    "        continue\n",
    "    # Iterates through each epoch's results in the history.\n",
    "    # Uses epoch_stride to select only every nth epoch (to avoid too many visualizations).\n",
    "    # Always includes the last epoch regardless of the stride.\n",
    "\n",
    "      samples_arr = [images_noisy, hist_el['y'][single_el_idx, 0], images]\n",
    "    # Creates an array of three sets of images to visualize side by side:\n",
    "       # The noisy input images\n",
    "       # The model's reconstructions for the current epoch\n",
    "       # The original clean images (ground truth)\n",
    "\n",
    "      ny = len(samples_arr)\n",
    "      nx = len(samples_arr[0])\n",
    "\n",
    "      plt.figure(figsize=(fig_scale*nx, fig_scale*ny))\n",
    "      # Calculates the dimensions of the visualization grid.\n",
    "      # Creates a figure with size proportional to the number of samples.\n",
    "\n",
    "        \n",
    "      m = mosaic(samples_arr)\n",
    "      # Uses the previously defined mosaic function to create a grid of all images.\n",
    "\n",
    "      plt.title(f'after epoch {int(epoch_idx)}')\n",
    "      plt.imshow(m, cmap='gray', vmin=-.5, vmax=.5)\n",
    "      # Adds a title showing which epoch this visualization represents.\n",
    "      # Displays the mosaic with a grayscale colormap and fixed value range.\n",
    "      # The vmin=-.5, vmax=.5 matches the normalized data range we've seen before.\n",
    "\n",
    "        \n",
    "      plt.tight_layout(pad=0.1, h_pad=0, w_pad=0)\n",
    "      plt.show()\n",
    "      plt.close()\n",
    "      # Ensures proper spacing in the figure.\n",
    "      # Displays the figure and then closes it to free memory.\n",
    "\n",
    "# This function creates a powerful visualization showing the progression of the model's reconstruction ability across epochs. Each visualization has three rows:\n",
    "# \n",
    "# The noisy inputs\n",
    "# The model's reconstructions\n",
    "# The original clean images (targets)\n",
    "# \n",
    "# This makes it easy to see how the model gradually learns to denoise and reconstruct the images over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwnutofatAc9"
   },
   "outputs": [],
   "source": [
    "# These are utility functions for working with trained models at different stages of training. Let me break them down:\n",
    "\n",
    "def run_on_trained(model, root_dir, run_fn, ep=None, model_filename=None):\n",
    "    \"\"\"\n",
    "    Helper function to excecute any function on model in state after `ep` training epoch\n",
    "    \"\"\"\n",
    "    # This function loads a model checkpoint and runs a specified function on it.\n",
    "    # Parameters:\n",
    "    # \n",
    "    # model: The neural network model instance\n",
    "    # root_dir: Directory containing saved model checkpoints\n",
    "    # run_fn: The function to run on the loaded model\n",
    "    # ep: Specific epoch to load (optional)\n",
    "    # model_filename: Specific checkpoint file to load (optional)\n",
    "\n",
    "    if model_filename is None:\n",
    "        if ep is not None:\n",
    "            model_filename = root_dir/f'model_{ep:03d}.pth'\n",
    "        else:\n",
    "            model_filename = sorted(list(root_dir.glob('*.pth')))[-1]  # last model state\n",
    "    # Determines which model checkpoint file to load:\n",
    "    # \n",
    "    # If a specific filename is provided, use that (in this case this code block would be skipped)\n",
    "    # If an epoch number is provided, construct the filename using a pattern\n",
    "    # If neither is provided, use the last checkpoint file (by alphabetical sorting)\n",
    "    # The code uses pathlib's Path objects for file handling (using / for path joining)\n",
    "\n",
    "    \n",
    "    model_dict = torch.load(model_filename,weights_only=False)\n",
    "\n",
    "    model.load_state_dict(model_dict['model_state_dict'])\n",
    "\n",
    "    # Loads the saved model state from the specified file\n",
    "    # The weights_only=False parameter indicates to load the full state dictionary (not just weights)\n",
    "    # Restores the model parameters from the saved state dictionary\n",
    "    \n",
    "\n",
    "    run_fn(model)\n",
    "    # Calls the provided function on the loaded model\n",
    "\n",
    "def run_on_all_training_history(model, root_dir, run_fn, n_ep=None):\n",
    "    \"\"\"\n",
    "    Helper function to excecute any function on model state after each of the training epochs\n",
    "    \"\"\"\n",
    "    # This function runs a specified function on multiple model checkpoints from different training epochs.\n",
    "    # Parameters:\n",
    "    # \n",
    "    # model: The neural network model instance\n",
    "    # root_dir: Directory containing saved model checkpoints\n",
    "    # run_fn: The function to run on each loaded model state\n",
    "    # n_ep: Specific number of epochs to process (optional)\n",
    "    \n",
    "    if n_ep is not None:\n",
    "        for ep in range(n_ep):\n",
    "            print(f'running on epoch {ep+1}/{n_ep}...')\n",
    "            run_on_trained(model, root_dir, run_fn, ep=ep)\n",
    "    # If a specific number of epochs is provided:\n",
    "    # \n",
    "    # Iterates through each epoch from 0 to n_ep-1\n",
    "    # Prints progress information\n",
    "    # Calls run_on_trained for each epoch\n",
    "    \n",
    "    else:\n",
    "        for model_filename in sorted(root_dir.glob('*.pth')):\n",
    "            print(f'running on checkpoint {model_filename}...')\n",
    "            run_on_trained(model, root_dir, run_fn, model_filename=model_filename)\n",
    "\n",
    "    # If no specific number of epochs is provided:\n",
    "    # \n",
    "    # Finds all .pth files in the root directory\n",
    "    # Sorts them (presumably by name, which would be by epoch if using the naming pattern)\n",
    "    # Processes each checkpoint file in order\n",
    "    \n",
    "    print(f'done')\n",
    "\n",
    "    # Prints a completion message when all checkpoints have been processed\n",
    "    # \n",
    "    # These utility functions make it easy to:\n",
    "    # \n",
    "    # Analyze a model at a specific point in its training history\n",
    "    # Run the same analysis across multiple stages of training\n",
    "    # Visualize or evaluate how the model's behavior changes over the course of training\n",
    "    # \n",
    "    # They're particularly useful for post-training analysis, debugging, and creating visualizations of model evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "evn49kSItAc9",
    "outputId": "72200c3b-a984-471b-8c87-97eb3a473e99"
   },
   "outputs": [],
   "source": [
    "# This code block initializes and tests the autoencoder model with a sample batch. \n",
    "# Let me explain it line by line:\n",
    "train_batch = next(iter(train_loader))\n",
    "# Gets the first batch from the training data loader without running a full epoch\n",
    "# iter(train_loader) creates an iterator from the data loader\n",
    "# next() retrieves the first element from that iterator (the first batch)\n",
    "xns, xs, ys = train_batch\n",
    "# Unpacks the batch into three components:\n",
    " # xns: The noisy input images\n",
    " # xs: The clean original images\n",
    " # ys: The class labels\n",
    "\n",
    "print('sample shapes:', xns.shape, xs.shape, ys.shape)\n",
    "# Prints the shapes of all three tensors to verify their dimensions\n",
    "# Likely shows something like [batch_size, 1, 28, 28] for the images\n",
    "\n",
    "in_size = xns.shape[1:]\n",
    "print(in_size)\n",
    "# Extracts the input size excluding the batch dimension\n",
    "# For MNIST, this would be [1, 28, 28] (channels, height, width)\n",
    "\n",
    "ae = AutoEncoder(input_size=in_size, code_size=10).to(device)\n",
    "# Creates an instance of the AutoEncoder model:\n",
    "# \n",
    "# input_size is set to the dimensions of the input data\n",
    "# code_size=10 defines the bottleneck dimension (the size of the encoded representation)\n",
    "# .to(device) moves the model to the appropriate device (CPU or GPU)\n",
    "\n",
    "y = ae(xns)\n",
    "# Performs a forward pass through the model with the noisy images\n",
    "# The model attempts to reconstruct the clean images from the noisy ones\n",
    "# Since return_z=False by default, this only returns the reconstructions\n",
    "\n",
    "print('output shape:', y.shape)\n",
    "# Prints the shape of the model's output\n",
    "# Should match the input shape, as the autoencoder reconstructs the original dimensions\n",
    "\n",
    "plot_im_samples(xns, is_torch=True)\n",
    "# Visualizes a few of the noisy input images using the previously defined function\n",
    "\n",
    "plot_im_samples(y, is_torch=True)\n",
    "# Visualizes the corresponding reconstructed images\n",
    "# This allows comparing the model's initial reconstructions before training -> The reconstructions are \n",
    "# just noise because the model has not been trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "vVMY2jaA62tN",
    "outputId": "0ab46f32-dbd8-4f4c-a507-2f13f61f207a"
   },
   "outputs": [],
   "source": [
    "# This code compares the pixel value distributions of an input image and its reconstruction. \n",
    "# Here's what each line does:\n",
    "\n",
    "x = xns[0]# - y[1]\n",
    "# Selects the first image from the batch of noisy inputs.\n",
    "# Note that there's a commented-out subtraction (# - y[1])\n",
    "\n",
    "d = y[0]# - y[1]\n",
    "# Selects the first image from the batch of reconstructed outputs.\n",
    "# Again, there's a commented-out subtraction\n",
    "\n",
    "im0 = x[0].detach().cpu().numpy()\n",
    "# Takes the first channel of the selected input image\n",
    "# Detaches it from the computation graph (no gradients needed)\n",
    "# Moves it to CPU if it was on GPU\n",
    "# Converts it to a NumPy array\n",
    "\n",
    "im1 = d[0].detach().cpu().numpy()\n",
    "# Does the same conversion process for the reconstructed image\n",
    "\n",
    "# plt.imshow(im, cmap='gray', vmin=-1, vmax=1)\n",
    "# This is a commented-out visualization that would display the image\n",
    "\n",
    "bins = np.linspace(-1, 1, 100)\n",
    "# Creates 100 evenly spaced histogram bins from -1 to 1\n",
    "# This range is chosen to match the expected range of pixel values\n",
    "\n",
    "plt.hist(im0.flatten(), bins, alpha=0.3);\n",
    "# Creates a histogram of all pixel values in the input image\n",
    "# flatten() converts the 2D image to a 1D array\n",
    "# alpha=0.3 makes the histogram semi-transparent\n",
    "\n",
    "plt.hist(im1.flatten(), bins, alpha=0.3);\n",
    "# Creates a histogram of all pixel values in the reconstructed image\n",
    "# Using the same bins and transparency\n",
    "# Overlaid on the same plot as the input image histogram\n",
    "\n",
    "\n",
    "# This visualization allows comparing the distribution of pixel values between \n",
    "# the noisy input and the reconstruction. It helps assess how well the autoencoder \n",
    "# is preserving the overall pixel value distribution and whether \n",
    "# it's correctly mapping values from the input distribution to the expected output distribution.\n",
    "# The semi-transparent overlapping histograms make it easy to see differences \n",
    "# in how pixel values are distributed between the original and reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4i9tkmotAc_",
    "outputId": "9e62b387-d75e-4961-f119-a67c3e836820"
   },
   "outputs": [],
   "source": [
    "ae.get_n_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUG1JVSCtAc_"
   },
   "source": [
    "# Simple Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSylMQQItAc_"
   },
   "source": [
    "First we train create an autoencoder with 5 latent variables for MNIST dataset.\n",
    "Each image is 28x28 pixels.\n",
    "We start from image data since it's easy to interpret and judge the reconstruction quality visually, but the very same applies to other data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMBFRdOttAdA"
   },
   "outputs": [],
   "source": [
    "# This code sets up the final model configuration and prepares the sample data for training:\n",
    "\n",
    "CODE_SIZE = 5\n",
    "# Sets the dimensionality of the encoded representation (bottleneck) to 5\n",
    "# This is smaller than the previous test where code_size was 10, \n",
    "# creating a more compressed representation\n",
    "\n",
    "\n",
    "NOISE_RATE = 0\n",
    "# Sets the noise rate for the denoising autoencoder to 0\n",
    "# This means no artificial noise will be added, making it \n",
    "# function as a standard autoencoder rather than a denoising one\n",
    "\n",
    "MODEL_NAME = 'ae_model'\n",
    "# Assigns a name to the model, likely used for saving checkpoints and organizing results\n",
    "\n",
    "model = AutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
    "# Creates a new instance of the AutoEncoder with:\n",
    "# \n",
    "# The previously determined input size (from the shape of the data)\n",
    "# The newly defined CODE_SIZE of 5\n",
    "# Placed on the appropriate device (CPU or GPU)\n",
    "\n",
    "samples = get_samples(valid_loader)\n",
    "# Calls the previously defined get_samples function to create a balanced set of \n",
    "# validation samples\n",
    "# These samples will be used to monitor reconstruction quality during training\n",
    "# The function selects representative samples from each class for visualization\n",
    "# \n",
    "# This code block is preparing the final model configuration before training. \n",
    "# It's worth noting that with NOISE_RATE set to 0, this will train a standard \n",
    "# autoencoder rather than a denoising autoencoder, despite the earlier code \n",
    "# being set up to handle noise addition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfsArKpntAdA",
    "outputId": "3a07514b-05a6-4b6f-893b-0e6597e06ba7"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-lP40zxtAdC"
   },
   "source": [
    "Train the model for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "983dbd1084cd43c1835a398d9a51d22e",
      "474e9082a92c45d295fcc8faa09fe4af",
      "f793438494ab4d27acb3c57705c7bffd",
      "7d7ef7296f0a41049bf1c81a65987a4c",
      "32b5a64c38474eb3b05eb1cde1994927",
      "b5eda53205b14617b72c40919dc33d65",
      "d759c3f33a6342ca838aa7935997ceea",
      "1abbc6518ef7445bbff2559b1a7cca2f",
      "cb8f00dac91e47b3b81084de7ac52e62",
      "c4ea9a4bb14e4df19d105c81ce6f2e6d",
      "80dabca945a246a288c9556aeb303da4"
     ]
    },
    "id": "6N1cGSNCtAdN",
    "outputId": "d1f719aa-6056-40c3-b088-9a2dd549209a"
   },
   "outputs": [],
   "source": [
    "# train the autoencoder model, for N_EPOCHS epochs,\n",
    "# save history of loss values for training and validation sets,\n",
    "# history of validation samples evolution, and model weights history,\n",
    "\n",
    "\n",
    "# This code implements the complete training loop for the autoencoder. \n",
    "# Let me break it down:\n",
    "\n",
    "N_EPOCHS = 50\n",
    "LR = 0.0009\n",
    "# Sets the number of training epochs to 50\n",
    "# Sets the learning rate for the Adam optimizer to 0.0009\n",
    "\n",
    "model_root = pl.Path(MODEL_NAME)\n",
    "model_root.mkdir(exist_ok=True)\n",
    "# Creates a directory path for saving model checkpoints using the MODEL_NAME ('ae_model')\n",
    "# Makes sure the directory exists (creates it if it doesn't)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "# Creates an Adam optimizer to update the model parameters\n",
    "# Adam is an adaptive learning rate optimization algorithm well-suited for deep learning\n",
    "\n",
    "# implement loss explicitly\n",
    "loss = nn.MSELoss()\n",
    "# Defines the loss function as Mean Squared Error (MSE)\n",
    "# This measures the average squared difference between the reconstructed and target images\n",
    "\n",
    "# train the model\n",
    "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
    "sample_history = {}\n",
    "# Creates dictionaries to store training metrics and sample reconstructions\n",
    "# history tracks training and validation losses across epochs\n",
    "# sample_history will store sample reconstruction results at each epoch\n",
    "\n",
    "pbar = tqdm.tqdm(range(N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
    "# Creates a progress bar for tracking the training process\n",
    "# Will show the current epoch and update metrics during training\n",
    "\n",
    "for epoch_idx in pbar:\n",
    "# Starts the main training loop that runs for N_EPOCHS iterations\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # Initializes the epoch loss accumulator\n",
    "    # Sets the model to training mode (enables dropout, batch normalization updates, etc.)\n",
    "    \n",
    "    for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(noisy_data)\n",
    "        loss_value = loss(output, data)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value.detach().cpu().item()\n",
    "    # Iterates through all batches in the training dataset\n",
    "    # For each batch:\n",
    "    # \n",
    "        # Zeros out previous gradients\n",
    "        # Passes the noisy input through the model\n",
    "        # Calculates the MSE loss between the reconstruction and clean data\n",
    "        # Computes gradients via backpropagation\n",
    "        # Updates model parameters using the optimizer\n",
    "        # Accumulates the loss value for epoch-level reporting\n",
    "    \n",
    "    epoch_loss /= len(train_loader)\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['epoch'].append(epoch_idx)\n",
    "    # update progress bar\n",
    "\n",
    "    # Calculates the average loss for the epoch\n",
    "    # Records the loss and epoch number in the history\n",
    "\n",
    "    # evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
    "            output = model(noisy_data)\n",
    "            loss_value = loss(output, data)\n",
    "            val_loss += loss_value.detach().cpu().item()\n",
    "        val_loss /= len(valid_loader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "    # Sets the model to evaluation mode (disables dropout, etc.)\n",
    "    # Disables gradient calculation for efficiency\n",
    "    # Computes the validation loss on the entire validation set\n",
    "    # Records the average validation loss in the history\n",
    "    \n",
    "    pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
    "    # evaluate on samples\n",
    "    # Updates the progress bar with current epoch, training loss, and validation loss\n",
    "    \n",
    "    sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
    "    # This saves the reconstructions and the latent space thanks to\n",
    "    # the eval_on_samples function where in the application of the \n",
    "    # model to the evaluation data the return_z parameter is set \n",
    "    # to true: \n",
    "    # with torch.no_grad():\n",
    "    #     yz = ae_model(xns, return_z=True)\n",
    "    #     yz = [el.detach().cpu().numpy() for el in yz]\n",
    "# \n",
    "    #     y = yz[0]\n",
    "    #     z = yz[1:]\n",
    "    \n",
    "    # The output of eval_on_samples looks like this: \n",
    "\n",
    "    # sample_res = {'z': z, 'y': y, 'epoch': epoch}\n",
    "\n",
    "    \n",
    "    sample_history[epoch_idx] = sample_res\n",
    "    # Evaluates the model on the sample images\n",
    "    # Stores reconstructions for later visualization\n",
    "\n",
    "    # save model weights\n",
    "    torch.save({\n",
    "                'epoch': epoch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss\n",
    "                }, model_root/f'model_{epoch_idx:03d}.pth')\n",
    "\n",
    "    # Saves a checkpoint of the model at each epoch\n",
    "    # The checkpoint includes:\n",
    "    # \n",
    "    # Current epoch number\n",
    "    # Model parameters\n",
    "    # Optimizer state (allows resuming training)\n",
    "    # Loss function\n",
    "    # \n",
    "    # \n",
    "    # Uses a formatted filename with padded epoch number (e.g., 'model_001.pth')\n",
    "# \n",
    "# This is a complete training pipeline that not only trains the model \n",
    "# but also tracks metrics, evaluates on validation data, \n",
    "# and creates visualizations to monitor progress - \n",
    "# all while saving checkpoints for later analysis or resuming training.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq3ZYMHrtAdO"
   },
   "source": [
    "Plot loss function evolution during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "s1F-fhnitAdP",
    "outputId": "3708e266-1af0-414a-e333-7b3c5bcfff47"
   },
   "outputs": [],
   "source": [
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuIjJna0tAdU"
   },
   "source": [
    "Let's visually compare network's output with the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c0HxF-QktAdU",
    "outputId": "f82361fa-c16a-496b-cfa8-a6e5081b0d36"
   },
   "outputs": [],
   "source": [
    "plot_samples(sample_history, samples=samples, epoch_stride=5, fig_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzL1wwdEHPfB",
    "outputId": "060260c6-7a3f-4a71-b120-ba1affb9aef4"
   },
   "outputs": [],
   "source": [
    "\n",
    "sample_history[49]['z'][0].T.shape  # 320 samples (32 each class) x 5 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "Co7xehyUGsCx",
    "outputId": "c7863c55-16af-416b-88e4-0b0b315bc3af"
   },
   "outputs": [],
   "source": [
    "for d in sample_history[49]['z'][0].T:\n",
    "  plt.hist(d, 100, alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-YdzZ2QDwLg"
   },
   "source": [
    "# Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15yh2Cfvh9RZ"
   },
   "source": [
    "In denoising AE we create noisy samples and ask the model to reconstruct original (clean) samples. Here 20% of pixels will be corrupted.\n",
    "This forces the model to learn more robust representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r45WCm8FtAdW"
   },
   "outputs": [],
   "source": [
    "# This code sets up a new model configuration, \n",
    "# specifically changing it from a standard autoencoder \n",
    "# to a denoising autoencoder:\n",
    "\n",
    "CODE_SIZE = 5\n",
    "# Keeps the dimensionality of the encoded representation \n",
    "# (bottleneck) at 5, same as before\n",
    "\n",
    "NOISE_RATE = 0.2\n",
    "# Sets the noise rate to 0.2, meaning approximately 20% of pixels \n",
    "# in each input image will be corrupted with noise\n",
    "# This is the key change that transforms the model from a standard \n",
    "# autoencoder to a denoising autoencoder\n",
    "# With this setting, the model will learn to reconstruct clean images \n",
    "# from artificially corrupted ones\n",
    "\n",
    "MODEL_NAME = 'dae_model'\n",
    "# Changes the model name to 'dae_model' (denoising autoencoder model) to distinguish it from the previous 'ae_model'\n",
    "# This will create a separate directory for saving checkpoints of this model\n",
    "\n",
    "model = AutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
    "# Creates a fresh instance of the AutoEncoder with the same architecture as before\n",
    "# Uses the previously determined input size and the CODE_SIZE of 5\n",
    "# Moves the model to the appropriate device (CPU or GPU)\n",
    "\n",
    "samples = get_samples(valid_loader)\n",
    "# Regenerates the validation samples for monitoring training progress\n",
    "# Since NOISE_RATE has changed, these samples will now include the 20% noise corruption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZgXG8vQtAdW",
    "outputId": "e037ef04-9329-478a-c62e-a512f23ae7ed"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N6i-gXjtAdX"
   },
   "source": [
    "Train the model for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "00e2ab4b7e60439aa1bd61bdf4781bb8",
      "8f5f98a52e824aeb921b99cb6f3527bc",
      "c4f4e17e87904108b4442cdb36443429",
      "f3e816556c95470794f40759a8d8c58e",
      "508a0fc496f445dbb37a177c3b2ff4bf",
      "7d6cf4cf803745e4b959b89eb3244256",
      "deeec5762ea54fbcb136e47696434be9",
      "35d9922f836f4fd2b0cdfe447af09bca",
      "638d985fe9da4d2aa7d34447049be1f6",
      "94baa119ed9845928991ca3440029131",
      "85c0efd7ac52436a83aeac87dc34f960"
     ]
    },
    "id": "-UKAEm9RtAdc",
    "outputId": "74a883cb-efbc-4484-aa1e-43ced398f00c"
   },
   "outputs": [],
   "source": [
    "# train the autoencoder model, for N_EPOCHS epochs,\n",
    "# save history of loss values for training and validation sets,\n",
    "# history of validation samples evolution, and model weights history,\n",
    "\n",
    "N_EPOCHS = 15\n",
    "LR = 0.0009\n",
    "\n",
    "\n",
    "model_root = pl.Path(MODEL_NAME)\n",
    "model_root.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# implement loss explicitly\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# train the model\n",
    "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
    "sample_history = {}\n",
    "\n",
    "pbar = tqdm.tqdm(range(N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
    "for epoch_idx in pbar:\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(noisy_data)\n",
    "        loss_value = loss(output, data)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value.detach().cpu().item()\n",
    "    epoch_loss /= len(train_loader)\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['epoch'].append(epoch_idx)\n",
    "    # update progress bar\n",
    "\n",
    "    # evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
    "            output = model(noisy_data)\n",
    "            loss_value = loss(output, data)\n",
    "            val_loss += loss_value.detach().cpu().item()\n",
    "        val_loss /= len(valid_loader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "    pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
    "    # evaluate on samples\n",
    "    sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
    "    sample_history[epoch_idx] = sample_res\n",
    "\n",
    "    # save model weights\n",
    "    torch.save({\n",
    "                'epoch': epoch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss\n",
    "                }, model_root/f'model_{epoch_idx:03d}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqyRCzactAdo"
   },
   "source": [
    "Plot loss function evolution during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "lgqjO5RPtAdp",
    "outputId": "b51c0f0d-1b50-46f2-8494-778ed9fa6f86"
   },
   "outputs": [],
   "source": [
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmD1KtcltAdr"
   },
   "source": [
    "Visualise evolution of reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 788
    },
    "id": "Bs-PtmOAtAdr",
    "outputId": "aed93d31-a447-4b33-8c6c-fbc3f500416d"
   },
   "outputs": [],
   "source": [
    "plot_samples(sample_history, samples=samples, epoch_stride=5, fig_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to access the latent space (conversation with Claude):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent space is already saved in the sample history (return_z parameter in the model application (part of the eval_on_samples function) is set to True). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, one could access the latent space by applying the trained model again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Methods to produce the latent space using the trained model (according to Claude): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using the encode() method directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a batch of images\n",
    "with torch.no_grad():\n",
    "    encoded_batch = model.encode(image_batch)  # image_batch should be on the same device as the model\n",
    "\n",
    "# Convert to numpy for visualization if needed\n",
    "encoded_batch_np = encoded_batch.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using the forward() method with return_z=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    reconstructed, encoded = model(image_batch, return_z=True)\n",
    "    \n",
    "encoded_np = encoded.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. For the entire validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encoded representations for all validation data\n",
    "all_encoded = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
    "        encoded = model.encode(data)  # Using clean images, not noisy ones\n",
    "        \n",
    "        all_encoded.append(encoded.detach().cpu().numpy())\n",
    "        all_labels.append(target.detach().cpu().numpy())\n",
    "        \n",
    "all_encoded = np.concatenate(all_encoded, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4saA2FcDFcMq"
   },
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZpzOK_OtsUu"
   },
   "source": [
    "Use autoencoder to find outliers:\n",
    "1. preform reconstruciton for validation dataset\n",
    "2. evaluate reconstruction loss and plot distrubution ()\n",
    "3. visualise poorely reconstructed samples\n",
    "\n",
    "Extra/Homework:\n",
    "Perform 2D UMAP embedding of the raw data points and image encoding (variables in latent space) for validation set, plot it and highlight outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tu5h3HaT6aCN"
   },
   "source": [
    "# Convolutional autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uf1wpycv6djD"
   },
   "source": [
    "Instead of fully connected layers we can use strided convolutional layers in encoder, and transposed convolutions in decoder.\n",
    "This model will have less parameters due to the weight sharing, thus easier to train.\n",
    "\n",
    "After upscaling the image size will be a bit bigger then original, so we also crop reconstruction to the input image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTsTRZ-atAdu"
   },
   "outputs": [],
   "source": [
    "class ConvolutionalAutoEncoder(AutoEncoder):\n",
    "# This class inherits from the AutoEncoder base class we discussed earlier, \n",
    "# extending it to use convolutional layers instead of fully connected layers.\n",
    "    def __init__(self, input_size, code_size):\n",
    "        self.input_size = list(input_size)  # shape of data sample\n",
    "\n",
    "        self.hidden_size = 32*2*2\n",
    "\n",
    "        self.code_size = code_size  # code size\n",
    "\n",
    "        super(ConvolutionalAutoEncoder, self).__init__(input_size, code_size)\n",
    "        # Initializes with the same parameters as the base class\n",
    "        # Sets hidden_size to 128 (32×2×2), which will be the size of the flattened representation before the final encoding\n",
    "        # Calls the parent class initializer, but will override the encoder and decoder definitions\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1,   8, 3, padding=1, stride=1), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(8,   8, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(8,  16, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(16, 16, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(16, 32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "\n",
    "            # Defines a series of convolutional layers for the encoder\n",
    "            # Each layer increases the number of feature maps while reducing spatial dimensions\n",
    "            # Starting with 1 channel (grayscale input), increasing to 8, then 16, then 32 channels\n",
    "            # Uses padding=1 to maintain spatial dimensions before downsampling\n",
    "            # Uses stride=2 in most layers to perform downsampling (reducing spatial dimensions by half)\n",
    "            # Each convolution is followed by LeakyReLU activation with negative_slope=0.3\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(self.hidden_size, self.hidden_size//8), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Linear(self.hidden_size//8, self.code_size),\n",
    "            # nn.Tanh(),\n",
    "            # After the convolutional layers, flattens the 3D feature maps into a 1D vector\n",
    "            # Applies two fully connected layers to reduce dimensions to the final code_size\n",
    "            # The final Tanh activation is commented out\n",
    "            \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.code_size, self.hidden_size), nn.LeakyReLU(negative_slope=0.3),\n",
    "\n",
    "            nn.Unflatten(1, (32, 2, 2)),\n",
    "            # The decoder starts with a fully connected layer to expand from code_size to hidden_size\n",
    "            # Reshapes the 1D vector back to a 3D feature map with shape (32, 2, 2)\n",
    "\n",
    "            nn.ConvTranspose2d(32, 16, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.ConvTranspose2d(16, 16, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.ConvTranspose2d(16,  8, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.ConvTranspose2d(8,   8, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "\n",
    "            # Uses transposed convolutions (also called deconvolutions) to upsample the feature maps\n",
    "            # Mirrors the encoder structure in reverse, gradually decreasing channels and increasing spatial dimensions\n",
    "            # output_padding=1 helps ensure the spatial dimensions match the original input after upsampling\n",
    "            \n",
    "            nn.Conv2d(8, 1, 3, padding=1, stride=1), nn.Tanh(),\n",
    "            # Final convolution layer produces a single channel output (grayscale image)\n",
    "            # Tanh activation constrains the output values to [-1, 1] range\n",
    "        )\n",
    "\n",
    "    def decode(self, z):\n",
    "        reconstruction = self.decoder(z)\n",
    "        reconstruction = reconstruction[:, :, 2:-2, 2:-2]\n",
    "        return reconstruction\n",
    "\n",
    "        # Overrides the parent class's decode method\n",
    "        # Applies the decoder to the latent representation\n",
    "        # Crops the reconstructed image by removing 2 pixels from each side\n",
    "        # This cropping likely compensates for any dimension mismatches caused by the convolution/deconvolution operations\n",
    "\n",
    "# This convolutional architecture is much more powerful for image data than the fully connected version, as it:\n",
    "# \n",
    "# 1. Preserves spatial relationships in the data\n",
    "# 2. Uses parameter sharing for efficiency\n",
    "# 3. Can learn hierarchical features (edges, textures, patterns)\n",
    "# 4. Generally results in better reconstructions and more meaningful latent representations for image data\n",
    "# \n",
    "# The progression from a simple fully connected autoencoder to this convolutional version is a common step in improving deep learning models for image data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUZaXBP5tAdu"
   },
   "source": [
    "Thus we can try to reduce sise of the latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySMlIeMdtAdv"
   },
   "outputs": [],
   "source": [
    "CODE_SIZE = 2\n",
    "NOISE_RATE = 0.2\n",
    "MODEL_NAME = 'cdae_model'\n",
    "model = ConvolutionalAutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
    "\n",
    "samples = get_samples(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hkKRmFfyfZu",
    "outputId": "54b2e3e6-f372-4e80-f498-f8ba983ded9c"
   },
   "outputs": [],
   "source": [
    "xns = torch.tensor(samples['images_noisy']).to(device)\n",
    "print(xns.shape)\n",
    "zs = model.encode(xns)\n",
    "ys = model(xns)\n",
    "print(zs.shape)\n",
    "print(ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIexAmnYtAdw",
    "outputId": "22d54c21-4ae5-4ba2-e15b-cec5bcb434f6"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdqOdtc-tAdw",
    "outputId": "678f05e7-9a62-408b-8d43-dce8d47d9657"
   },
   "outputs": [],
   "source": [
    "model.get_n_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOsrmRT5diiN"
   },
   "outputs": [],
   "source": [
    "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
    "sample_history = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAqBhNcAtAdw"
   },
   "source": [
    "Train the model for 70 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "1623c03a91fa4f80b9109e281bda1a32",
      "b321467d620a4a908cd906a6c6d28a17",
      "fae564eac5eb4d95a5fda6b3e6b083de",
      "c705c3f1b21e4f41a75fbe5319e48383",
      "48d161e77c1d4e40beecb0f263d54763",
      "55f4b904d9404e8cbf9e1a5c8c934436",
      "7ee5ddf3b47b46feaa3eeb372f489c98",
      "4b48e00446464125ad5f139ba9e68c7b",
      "8f47b7e2b591402eba7cafdfd404a01c",
      "0fedb0e6df744ad7b32dd5f14a493b61",
      "9c09c1eba258404f870ba0895b638570"
     ]
    },
    "id": "NBP1oJqKtAdx",
    "outputId": "a390e2dc-26fa-4f6b-8f30-b90a999da8ce"
   },
   "outputs": [],
   "source": [
    "# train the autoencoder model, for N_EPOCHS epochs,\n",
    "# save history of loss values for training and validation sets,\n",
    "# history of validation samples evolution, and model weights history,\n",
    "\n",
    "N_EPOCHS = 70\n",
    "LR = 0.0004\n",
    "\n",
    "\n",
    "model_root = pl.Path(MODEL_NAME)\n",
    "model_root.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# implement loss explicitly\n",
    "loss = nn.L1Loss()\n",
    "\n",
    "# train the model\n",
    "#history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
    "#sample_history = {}\n",
    "\n",
    "pbar = tqdm.tqdm(range(0, N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
    "for epoch_idx in pbar:\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(noisy_data)\n",
    "        loss_value = loss(output, data)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value.detach().cpu().item()\n",
    "    epoch_loss /= len(train_loader)\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['epoch'].append(epoch_idx)\n",
    "    # update progress bar\n",
    "\n",
    "    # evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
    "            output = model(noisy_data)\n",
    "            loss_value = loss(output, data)\n",
    "            val_loss += loss_value.detach().cpu().item()\n",
    "        val_loss /= len(valid_loader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "    pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
    "    # evaluate on samples\n",
    "    sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
    "    sample_history[epoch_idx] = sample_res\n",
    "\n",
    "    # save model weights\n",
    "    torch.save({\n",
    "                'epoch': epoch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss\n",
    "                }, model_root/f'model_{epoch_idx:03d}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccaQpXXptAdy"
   },
   "source": [
    "Plot loss function evolution during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "QYXjiKE1tAdy",
    "outputId": "5c6bf66f-a72d-41fb-e649-6de613d7d6b1"
   },
   "outputs": [],
   "source": [
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSvnZCWvtAd0"
   },
   "source": [
    "Visualise evolution of reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "id": "cRwLeOzGtAd0",
    "outputId": "41686720-e779-47bd-a22a-2dbfd7bdff2e"
   },
   "outputs": [],
   "source": [
    "plt.hist(samples['images'].flatten(), bins=100, log=True);\n",
    "for k in ['y', 'z']:\n",
    "    #print(sample_history[0][k].shape)\n",
    "    plt.hist(np.array(sample_history[0][k]).flatten(), bins=100, log=True, alpha=0.3);\n",
    "    plt.hist(np.array(sample_history[epoch_idx-1][k]).flatten(), bins=100, log=True, alpha=0.3);\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GkLWD8yXtAd0",
    "outputId": "b1267f19-a8b4-4c29-c6d7-68ee38f935ec"
   },
   "outputs": [],
   "source": [
    "plot_samples(sample_history, samples=samples, epoch_stride=5, fig_scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVUXN5Jq7kx9"
   },
   "source": [
    "## Latent space\n",
    "\n",
    "So far we looked just on the output: the model sort of does the job. But what does it learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mVBP8b0t-Wl"
   },
   "source": [
    "First let's use animation to visualize reconstruction evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wzDeCVObTlI"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Jupyter notebook cell magic to suppress output (useful for code that might produce verbose output)\n",
    "\n",
    "single_el_idx = samples['single_el_idx']\n",
    "images_noisy = samples['images_noisy'][single_el_idx, 0]\n",
    "images = samples['images'][single_el_idx, 0]\n",
    "# Extracts the indices for the selected sample images\n",
    "# Gets the noisy input images and original clean images for these indices\n",
    "# The , 0 selects the first channel (since these are grayscale images)\n",
    "\n",
    "smpl_ims = []\n",
    "for epoch_idx, hist_el in sample_history.items():\n",
    "    samples_arr = [images_noisy, hist_el['y'][single_el_idx, 0], images]\n",
    "    smpl_ims.append(samples_arr)\n",
    "# Creates a list to store image arrays for each epoch\n",
    "# For each epoch in the training history:\n",
    "    # \n",
    "    # Creates an array containing [noisy inputs, reconstructions, original images]\n",
    "    # Adds this array to the list\n",
    "\n",
    "ny, nx = len(smpl_ims[0]), len(smpl_ims[0][0])\n",
    "# Determines the number of rows (3: noisy, reconstructed, original) and columns (number of samples)\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above animations use JavaScript\n",
    "# Sets matplotlib to use JavaScript for HTML animations in Jupyter\n",
    "\n",
    "s=1\n",
    "fig = plt.figure(figsize=(s*nx, s*ny))\n",
    "# Creates a figure with size proportional to the number of images\n",
    "\n",
    "m = mosaic(smpl_ims[0])\n",
    "\n",
    "ttl = plt.title(f'after epoch {int(0)}')\n",
    "# plot 0th epoch - 0th frame\n",
    "imsh = plt.imshow(m, cmap='gray', vmin=-0.5, vmax=0.5)\n",
    "# Creates the initial frame of the animation using the first epoch's images\n",
    "# Uses the mosaic function to arrange the images in a grid\n",
    "# Sets grayscale colormap with value range [-0.5, 0.5]\n",
    "\n",
    "# this function will be called to render each of the frames\n",
    "def animate(i):\n",
    "    m = mosaic(smpl_ims[i])\n",
    "    imsh.set_data(m)\n",
    "\n",
    "    ttl.set_text(f'after epoch {i}')\n",
    "\n",
    "    return imsh\n",
    "\n",
    "# Defines a function to update the plot for each frame of the animation\n",
    "# Creates a mosaic of images for the current epoch\n",
    "# Updates the image data and title text\n",
    "# Returns the updated image object\n",
    "\n",
    "# create animation\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(smpl_ims))\n",
    "\n",
    "# Creates an animation that calls the animate function for each epoch\n",
    "# The result is a dynamic visualization showing how reconstructions evolve throughout training\n",
    "\n",
    "# This animation provides an intuitive way to observe the autoencoder's learning \n",
    "# progress, allowing you to see how the model gradually improves at reconstructing \n",
    "# the original images from the noisy inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "p25DtqevVw7q",
    "outputId": "3807e527-4049-4676-a151-ee7d1d8470d2"
   },
   "outputs": [],
   "source": [
    "# display animation\n",
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Qu6J9kGuEt9"
   },
   "source": [
    "And let's see evolution of the latent representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngMUudwdDpg5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above, uses JavaScript\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "# Suppresses output with %%capture\n",
    "# Sets matplotlib to use JavaScript for HTML animations\n",
    "# Creates a square figure with size 8×8 inches\n",
    "\n",
    "labels = samples['labels']\n",
    "epochs = sorted(sample_history.keys())\n",
    "z_res = [sample_history[ep]['z'][0] for ep in epochs]\n",
    "# Gets the digit labels from the samples dictionary\n",
    "# Creates a sorted list of all epoch numbers\n",
    "# Extracts the latent space representations from each epoch\n",
    "\n",
    "scat = plt.scatter(z_res[0][:,0], z_res[0][:,1], c=labels, cmap=cm.rainbow)\n",
    "# Creates a scatter plot using the first two dimensions of the latent space from the first epoch\n",
    "# Colors the points according to their digit labels (0-9)\n",
    "# Uses the rainbow colormap to distinguish between different digits\n",
    "\n",
    "plt.xlim(-6.1, 6.1)\n",
    "plt.ylim(-6.1, 6.1)\n",
    "\n",
    "ax = plt.gca()\n",
    "legend1 = ax.legend(*scat.legend_elements(), title=\"digits\")\n",
    "ax.add_artist(legend1)\n",
    "ax.set_aspect('equal')\n",
    "ttl = plt.title(f'after epoch {0}')\n",
    "# Sets fixed axis limits for consistent visualization across frames\n",
    "# Gets the current axis\n",
    "# Adds a legend showing the mapping between colors and digit classes\n",
    "# Sets the aspect ratio to equal so circles appear as circles\n",
    "# Adds a title showing the current epoch\n",
    "\n",
    "def animate(i):\n",
    "    z = z_res[i]\n",
    "    scat.set_offsets(z)\n",
    "    ttl.set_text(f'after epoch {i}')\n",
    "    return scat\n",
    "\n",
    "# Defines a function to update the plot for each animation frame\n",
    "# Updates the scatter plot with the latent representations from the current epoch\n",
    "# Updates the title text with the current epoch number\n",
    "# Returns the updated scatter plot object\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(z_res))\n",
    "\n",
    "# Creates an animation that runs through all epochs\n",
    "\n",
    "# This animation shows how the model progressively learns to organize \n",
    "# the latent space, with points representing the same digit class gradually \n",
    "# clustering together. It's a powerful visualization that helps understand \n",
    "# how the autoencoder is learning meaningful representations and \n",
    "# separating different classes in the latent space, even though it's \n",
    "# trained in an unsupervised manner without using the labels for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 898
    },
    "id": "W-sYawGtV1le",
    "outputId": "6ba705a3-b9cf-4f75-8370-a98397eeee13"
   },
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qArUdx6YHzD"
   },
   "source": [
    "## Sampling from latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syOBR27sn6vm"
   },
   "source": [
    "But we can also use the trained model to generate samples based on the latent representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CLRUV7ad5P8p",
    "outputId": "32d534bd-b225-4bd0-8be4-95070216e211"
   },
   "outputs": [],
   "source": [
    "# This code generates and visualizes images from random points in the latent space \n",
    "# across different training epochs. Here's the breakdown:\n",
    "\n",
    "z0 = np.random.uniform(size=(25, CODE_SIZE))*12-6  # get 20 random points in 2D sampled from uniform distribution between 0 and 1\n",
    "# Creates 25 random points in the latent space\n",
    "# Scales the values to the range [-6, 6] (multiplies by 12 and subtracts 6)\n",
    "# This creates a uniform distribution of points across the latent space\n",
    "\n",
    "z0_t = torch.tensor(z0, dtype=torch.float32).to(device)\n",
    "# Converts the random points to a PyTorch tensor\n",
    "# Moves the tensor to the appropriate device (CPU or GPU)\n",
    "\n",
    "ims_all = []\n",
    "# Creates an empty list to store generated images from each epoch\n",
    "\n",
    "# this function will be called in saved model state after each training epoch\n",
    "def fn(ae):\n",
    "  with torch.no_grad():\n",
    "    ims = ae.decode(z0_t)\n",
    "    ims = ims.detach().cpu().numpy()\n",
    "    ims_all.append(ims)\n",
    "# Defines a function to be called on the model checkpoint from each epoch\n",
    "# Uses torch.no_grad() to disable gradient computation (for efficiency)\n",
    "# Decodes the random latent vectors to generate images\n",
    "# Converts the resulting images to NumPy arrays\n",
    "# Adds the batch of images to the collection\n",
    "\n",
    "run_on_all_training_history(model, model_root, fn)\n",
    "# Calls the run_on_all_training_history function we saw earlier\n",
    "# This runs the fn function on the model at each saved epoch\n",
    "# The function generates images from the same random latent points using \n",
    "# each epoch's model state\n",
    "\n",
    "\n",
    "ims_all = np.array(ims_all)\n",
    "print(ims_all.shape)\n",
    "# Converts the list of image batches to a NumPy array\n",
    "# Prints the shape of the array, likely [n_epochs, 25, 1, height, width]\n",
    "\n",
    "    # n_epochs: Number of training epochs\n",
    "    # 25: Number of random latent vectors\n",
    "    # 1: Number of channels (grayscale)\n",
    "    # height, width: Image dimensions\n",
    "\n",
    "ims_all = ims_all[:, :, 0, :, :]  # remove channel dimension\n",
    "# Removes the channel dimension from the array\n",
    "# This simplifies the array shape to [n_epochs, 25, height, width]\n",
    "# \n",
    "# This code allows you to visualize how the model's ability to generate images \n",
    "# from latent space evolves throughout training. It also helps understand \n",
    "# what different regions of the latent space represent. \n",
    "# Since you're using the same random points across all epochs, \n",
    "# you can see how the model's interpretation of those points changes as it learns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "bdGfJcLItAeC",
    "outputId": "7699e052-62ee-4c44-e38b-49239dba4f69"
   },
   "outputs": [],
   "source": [
    "plt.hist(ims_all[0].flatten(), bins=100, log=True);\n",
    "plt.hist(ims_all[-1].flatten(), bins=100, log=True, alpha=0.5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Edg10WV74629"
   },
   "outputs": [],
   "source": [
    "\n",
    "# This code creates an animation showing how the decoder's ability to generate \n",
    "# images from random latent vectors evolves over training epochs:\n",
    "\n",
    "%%capture\n",
    "fig = plt.figure(figsize=(20, 1.5))\n",
    "# Suppresses output with %%capture\n",
    "# Creates a wide, short figure to display the generated images in a row\n",
    "\n",
    "m = mosaic([ims_all[0]])\n",
    "# Uses the mosaic function to arrange the first epoch's images in a grid\n",
    "# Since ims_all[0] contains all 25 images from epoch 0, this creates \n",
    "# a single row of images\n",
    "\n",
    "imsh = plt.imshow(m, cmap='gray', vmin=0, vmax=1)\n",
    "ttl = plt.title(f'after epoch {int(0)}')\n",
    "# Displays the mosaic with a grayscale colormap\n",
    "# Sets the value range to [0, 1] for proper contrast\n",
    "# Adds a title showing the current epoch\n",
    "\n",
    "def animate(i):\n",
    "    m = mosaic([ims_all[i]])\n",
    "    imsh.set_data(m)\n",
    "    ttl.set_text(f'after epoch {i}')\n",
    "    return imsh\n",
    "# Defines a function to update the plot for each animation frame\n",
    "# Creates a mosaic of the generated images from the current epoch\n",
    "# Updates the image data and title\n",
    "# Returns the updated image object\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(ims_all))\n",
    "\n",
    "# Creates an animation that cycles through all epochs\n",
    "# \n",
    "# This visualization shows how the model learns to map points in the latent space \n",
    "# to meaningful images over time. Initially, the generated images may look \n",
    "# like noise or very basic patterns. As training progresses, they should gradually \n",
    "# transform into more recognizable digit-like shapes.\n",
    "# What makes this particularly interesting is that these images are generated \n",
    "# from random points in the latent space, not from encoded real images. \n",
    "# This demonstrates the generative capability of the autoencoder - its ability \n",
    "# to create new, plausible examples by sampling from the latent space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "qy49wsQJV5Ky",
    "outputId": "29305767-1455-4077-ada7-76d4519ba62f"
   },
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk1sA3cNFuLB"
   },
   "source": [
    "## Interpolation in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsyVm5m_F3ng"
   },
   "outputs": [],
   "source": [
    "# This code creates a regular grid of points in a 2D latent space for visualization \n",
    "# purposes. Here's what it does:\n",
    "\n",
    "n_samples = 10\n",
    "# Sets the number of samples per dimension to 10, which will create a 10×10 grid\n",
    "\n",
    "zs = np.meshgrid(np.linspace(-6, 6, n_samples),\n",
    "                 np.linspace(-6, 6, n_samples))\n",
    "# Creates two coordinate arrays using np.meshgrid\n",
    "# Each array represents one dimension of the latent space\n",
    "# np.linspace(-6, 6, n_samples) creates 10 equally spaced points from -6 to 6\n",
    "# Together, these form a regular 10×10 grid of coordinates in 2D space\n",
    "\n",
    "zs = np.stack(zs, axis=-1).reshape(-1, 2)\n",
    "# np.stack(zs, axis=-1) combines the two coordinate arrays into a single 3D array with shape [10, 10, 2]\n",
    "# .reshape(-1, 2) flattens the grid into a list of 100 points (10×10) with 2 coordinates each\n",
    "# The result is a 2D array with shape [100, 2]\n",
    "\n",
    "zs_t = torch.tensor(zs, dtype=torch.float32).to(device)\n",
    "# Converts the NumPy array to a PyTorch tensor\n",
    "# Ensures the data type is float32\n",
    "# Moves the tensor to the appropriate device (CPU or GPU)\n",
    "# \n",
    "# This grid of points is useful for creating a visualization of the \n",
    "# latent space manifold - by decoding each point in this regular grid, \n",
    "# you can see how the latent space is organized and how different regions \n",
    "# correspond to different types of generated images. This is commonly used \n",
    "# to create a \"latent space walk\" or \"manifold visualization\" that shows \n",
    "# how the generated images change as you move through the latent space in a structured way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "id": "eHivvVP0KaQd",
    "outputId": "fc601d87-3b44-4381-aea1-b0f01e4633a3"
   },
   "outputs": [],
   "source": [
    "def fn(ae):\n",
    "  with torch.no_grad():\n",
    "    ims_t = ae.decode(zs_t)\n",
    "    ims = ims_t.detach().cpu().numpy()\n",
    "    ims = ims[:, 0, :, :]  # remove channel dimension\n",
    "\n",
    "  sh = list(ims.shape)\n",
    "  ims = ims.reshape([n_samples, n_samples]+sh[1:])\n",
    "  plt.figure(figsize=(10, 10))\n",
    "\n",
    "  # here image order is reversed along y axis in `ims` to match y axis direction on the plot\n",
    "  # (y axis goes from bottom to top, while pixel order - from top to bottom)\n",
    "  plt.imshow(mosaic(ims[::-1]), vmin=0, vmax=1, cmap='gray')\n",
    "  plt.show()\n",
    "\n",
    "run_on_trained(model, model_root, fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phTJlLGx3a5H"
   },
   "source": [
    "## Exercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYaGkilctAeF"
   },
   "source": [
    "Work in 2 groups:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXr6-NGf6OW4"
   },
   "source": [
    "\n",
    "Option 1. Compare distribution in latent space for noise rate 0, 0.2, 0.6, 0.8.\n",
    "\n",
    "Option 2. Compare reconstruction depending on size of latent space: 2, 4, 8, 16, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vp-ZrvjUtAeG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBT5XbLztAeH"
   },
   "source": [
    "## Exercise 3: Writing latent Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A85koGGYtAeK"
   },
   "source": [
    "\n",
    "In this exercise, we will\n",
    "1. modify the CNN to be fully convolutional, with 4 downscaling layers (x16), 16 channels\n",
    "2. save dataset of latent states, see code below for example.\n",
    "\n",
    "When usuing VAE - save the Z nean and log std - separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iS4ZCW4tAeK"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pckl(obj, fname):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.DEFAULT_PROTOCOL)\n",
    "\n",
    "def load_pckl(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FPCKd8__tAeM",
    "outputId": "9f6c38af-2fa6-42d8-82a8-eab69126c791"
   },
   "outputs": [],
   "source": [
    "#load latest model checkpoint:\n",
    "run_on_trained(model, model_root, lambda m:None, ep=3)\n",
    "\n",
    "# dataset for latent DDM, file list:\n",
    "data_files = {\"train\": [], \"valid\": []}\n",
    "\n",
    "ds_root = pl.Path('data')\n",
    "ds_root.mkdir(exist_ok=True)\n",
    "\n",
    "# generate latent values per blocks of samples:\n",
    "block_sz = 5000\n",
    "for ds, sfx in zip([valid_dataset, train_dataset], ['valid', 'train']):\n",
    "  x = ds.data.numpy().reshape(-1, 1, 28, 28)\n",
    "  x = x/255.*2-1\n",
    "  l = ds.targets.numpy()\n",
    "\n",
    "  for i in range(0, len(x), block_sz):\n",
    "    x_b = x[i:i+block_sz]\n",
    "    l_b = l[i:i+block_sz]\n",
    "\n",
    "    x_b_t = torch.tensor(x_b, dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z_t = model.encode(x_b_t)\n",
    "        z_b = z_t.detach().cpu().numpy()\n",
    "\n",
    "    # save latent values and their shape and labels to a dataframe:\n",
    "    data_d = {\n",
    "    'z': [zi.flatten() for zi in z_b],\n",
    "    'shape': [zi.shape for zi in z_b],\n",
    "    'label': l_b\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data=data_d)\n",
    "\n",
    "    # save dataframe to a file and reference dictioanry:\n",
    "    fname = ds_root/f'df_z_{sfx}_{i}.pckl'\n",
    "    save_pckl(df, fname)\n",
    "    data_files[sfx].append(str(fname))\n",
    "\n",
    "save_pckl(data_files, ds_root/'data_files.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_olDFiXFK_M",
    "outputId": "45bf96e9-d4fa-4fd0-81b3-cd1a1e2bed40"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBIJwlWRtAeN"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137,
     "referenced_widgets": [
      "b58249e4ec5a467fb40c70917b0f2eaa",
      "8552e02fdd1b480bb09e64af54fb2829",
      "3cb558d03c524075abfc67a154ec4f0d",
      "3867dfbe772141b58ff8728a201f76dc",
      "7ed5c6c6f5c749b2849b203da467f8e2",
      "03b115e9fd9e4f6bacee75bf1e91735f",
      "3944d43006094c3ead030b6ffe505815",
      "3494b43c49c14b009a7e03ae6798f818",
      "5817af58e0c443f5bd226645b0157e78",
      "81d4444cb8414b56990835b2ed99677e",
      "7802e2b380da47a2979a6e4ec29da921",
      "05eed87241de4972a7acc70f48c3430c",
      "0d97d53e4453459ab1ed6f7e49c1d6d6",
      "7e7868cebe47479e82ea30763c86edca",
      "d607db3bc256454092cb1eb024901c20",
      "f03adde2b61040daa05a9a981512af51",
      "fb197b7ef6d6484e80a4f77f6f03b1e7",
      "8bc1b3f02e3543989bacf0aa882589e0",
      "ec4522af12ec4cdf80d3c29d425ac273",
      "0da5b7e0ce25441cba444e632ef62340",
      "94db787b4fe04f3a9aafccabc73e9f31",
      "9936fe021fa74ea2a3429cacbfb252cb"
     ]
    },
    "id": "ZJELUTmKtAeO",
    "outputId": "03020801-22fc-443b-9d32-dfbb165b6292"
   },
   "outputs": [],
   "source": [
    "# now we can load the dataset:\n",
    "data_files = load_pckl(ds_root/'data_files.pkl')\n",
    "lds = load_dataset('pandas', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BprdQfcEtAeU"
   },
   "outputs": [],
   "source": [
    "# convert to torch dataset:\n",
    "tds = lds.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5szuM5XtAeU",
    "outputId": "e55a78d3-c698-4230-df86-8541dbb18f84"
   },
   "outputs": [],
   "source": [
    "tds['train']['z'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_tWAYpah3aG"
   },
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnBr9bvW39zT"
   },
   "source": [
    "While the previous model learn to seaparate subpopulations in the latent space, there remains significant overlap & unpopulated regions.\n",
    "\n",
    "Varitional AE puts additional constraints on the distribution in the latent space and perform variational inference.\n",
    "\n",
    "(see pptx for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kshKMIOwwD9W"
   },
   "source": [
    "Here the encoder will predict `2 x n_code` values: means and logarithm of variance for each sample. Since these values live in $R^2$ - no activation function is used in last layer of the encoder.\n",
    "\n",
    "Then for reconstruction we will sample from this distribuition with a reparametrisation trick.\n",
    "\n",
    "The tecnically complex part - is to implement the custom loss function and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaKbgRDetAeW"
   },
   "outputs": [],
   "source": [
    "class VariationalConvolutionalAutoencoder(AutoEncoder):\n",
    "    def __init__(self, input_size, code_size):\n",
    "        super(VariationalConvolutionalAutoencoder, self).__init__(input_size, code_size)\n",
    "\n",
    "        # nn.LeakyReLU(negative_slope=0.3)\n",
    "        self.input_size = list(input_size)  # shape of data sample\n",
    "        self.npix = np.prod(self.input_size)\n",
    "\n",
    "        self.hidden_size = 64*1\n",
    "\n",
    "        self.code_size = code_size  # code size\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1,   16, 3, padding=1, stride=1), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(16,  16, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(16,  32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(32,  32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(32,  32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Conv2d(32,  64, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(self.hidden_size, 64), nn.LeakyReLU(negative_slope=0.3),\n",
    "\n",
    "            # we want values in all R, thus no activation function is applied. self.n_code values for mean + self.n_code for log(variance)\n",
    "            nn.Linear(64, self.code_size * 2),\n",
    "\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.code_size, 64), nn.LeakyReLU(negative_slope=0.3),\n",
    "            nn.Linear(64, self.hidden_size), nn.LeakyReLU(negative_slope=0.3),\n",
    "\n",
    "            nn.Unflatten(1, (64, 1, 1)),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 32, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 32, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 16, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(16, 1, 3, padding=1, stride=1), nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            # samle from standard normal distribution\n",
    "            eps = torch.randn((100, self.code_size))\n",
    "        return self.decode(eps)\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_mean, z_logvar = torch.split(z, split_size_or_sections=2, dim=1)\n",
    "        return z_mean, z_logvar\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        # reaparametrization trick: to sample z from N(mean, std):\n",
    "        # z = mean + std * eps, where eps sampled from N(0, 1)\n",
    "        eps = torch.randn_like(z_mean)\n",
    "        z_std = torch.exp(z_logvar * .5)\n",
    "        return eps * z_std + z_mean\n",
    "\n",
    "    def decode(self, z):\n",
    "        reconstruction = self.decoder(z)\n",
    "        reconstruction = reconstruction[:, :, 2:-2, 2:-2]\n",
    "        return reconstruction\n",
    "\n",
    "    def forward(self, x, return_z=False):\n",
    "        z_mean, z_logvar = self.encode(x)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        reconstruction = self.decode(z)\n",
    "        return (reconstruction, z_mean, z_logvar) if return_z else reconstruction\n",
    "\n",
    "    def forward_and_KL_loss(self, x, y):\n",
    "        reconstruction, z_mean, z_logvar = self(x, return_z=True)\n",
    "\n",
    "        # tf impl:\n",
    "        # loss_z_kl = 0.5 * tf.reduce_sum(tf.exp(z_logvar) + tf.square(z_mean) -1.0 - z_logvar, axis=1)  # KL divergence from N(0, 1) to N(z_mean, tf.exp(z_logvar * .5))\n",
    "        # loss_z_kl = tf.reduce_mean(loss_z_kl) / np.prod(self.data_size)  # but since we used mean in reconstruction loss - this term has to be normalized accordingly\n",
    "\n",
    "        # pytorch impl:\n",
    "        loss_z_kl = 0.5 * torch.sum(torch.exp(z_logvar) + torch.square(z_mean) -1.0 - z_logvar, dim=1)\n",
    "        loss_z_kl = torch.mean(loss_z_kl) / self.npix\n",
    "\n",
    "        return reconstruction, loss_z_kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHvSydQKtAeY"
   },
   "outputs": [],
   "source": [
    "CODE_SIZE = 2 # 50\n",
    "NOISE_RATE = 0.\n",
    "MODEL_NAME = 'vcae_model'\n",
    "model = VariationalConvolutionalAutoencoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
    "\n",
    "samples = get_samples(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j6ngCGuHtAed",
    "outputId": "fa77f5b4-ff4b-4c1d-c0a3-cbda40c763ac"
   },
   "outputs": [],
   "source": [
    "xns = torch.tensor(samples['images_noisy']).to(device)\n",
    "print(xns.shape)\n",
    "ys = model(xns)\n",
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Is6wGSR4tAee",
    "outputId": "345cf1bf-e273-4db3-f630-b01d31df863c"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09QZTchYtAej",
    "outputId": "7311bd25-9986-44ff-b715-92472fdc3ad2"
   },
   "outputs": [],
   "source": [
    "model.get_n_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8f9a49a562f341bbbb71a27a06a6ad1d",
      "4e04bb3c232a469bac1001aa5ac8a561",
      "db4cb3d76be84a52830ddb970ab3fe89",
      "5980b89f1166499ba6185311002463f2",
      "1179f40122794b168dc0665d77b3bd15",
      "a537b8225fb743db9c3054959b2c2234",
      "d8dfe23f85134fd6b02e07e2fc2aa25d",
      "6e0683251574417793ac741546fcafef",
      "5ec243527f6540009166f1147122d4f1",
      "0fd05eefe82d4e63b9fda9a6124e5153",
      "0676ba35b6194a078f53b30ad05f4991"
     ]
    },
    "id": "VzquUs8AtAek",
    "outputId": "e3bdc3bf-5f16-46c0-e831-6d1862202a35"
   },
   "outputs": [],
   "source": [
    "# train the autoencoder model, for N_EPOCHS epochs,\n",
    "# save history of loss values for training and validation sets,\n",
    "# history of validation samples evolution, and model weights history,\n",
    "\n",
    "N_EPOCHS = 20 # 20\n",
    "LR = 0.0009\n",
    "\n",
    "\n",
    "model_root = pl.Path(MODEL_NAME)\n",
    "model_root.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# implement loss explicitly\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# train the model\n",
    "history = {'loss': [], 'val_loss': [], 'rec_loss': [], 'rec_val_loss': [], 'kl_loss': [], 'kl_val_loss': [], 'epoch': []}\n",
    "sample_history = {}\n",
    "\n",
    "pbar = tqdm.tqdm(range(N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
    "for epoch_idx in pbar:\n",
    "    epoch_loss = 0\n",
    "    epoch_rec_loss = 0\n",
    "    epoch_kl_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        #output = model(noisy_data)\n",
    "        output, kl_loss = model.forward_and_KL_loss(noisy_data, data)\n",
    "        rec_loss = loss(output, data)\n",
    "        loss_value = rec_loss + kl_loss\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss_value.detach().cpu().item()\n",
    "        epoch_rec_loss += rec_loss.detach().cpu().item()\n",
    "        epoch_kl_loss += kl_loss.detach().cpu().item()\n",
    "\n",
    "    n_elements = len(train_loader)\n",
    "\n",
    "    epoch_loss /= n_elements\n",
    "    epoch_rec_loss /= n_elements\n",
    "    epoch_kl_loss /= n_elements\n",
    "\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['rec_loss'].append(epoch_rec_loss)\n",
    "    history['kl_loss'].append(epoch_kl_loss)\n",
    "\n",
    "    history['epoch'].append(epoch_idx)\n",
    "    # update progress bar\n",
    "\n",
    "    # evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        val_rec_loss = 0\n",
    "        val_kl_loss = 0\n",
    "\n",
    "        for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
    "            #output = model(noisy_data)\n",
    "            output, kl_loss = model.forward_and_KL_loss(noisy_data, data)\n",
    "            rec_loss = loss(output, data)\n",
    "            loss_value = rec_loss + kl_loss\n",
    "\n",
    "            val_loss += loss_value.detach().cpu().item()\n",
    "            val_rec_loss += rec_loss.detach().cpu().item()\n",
    "            val_kl_loss += kl_loss.detach().cpu().item()\n",
    "\n",
    "        val_loss /= len(valid_loader)\n",
    "        val_rec_loss /= len(valid_loader)\n",
    "        val_kl_loss /= len(valid_loader)\n",
    "\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['rec_val_loss'].append(val_rec_loss)\n",
    "        history['kl_val_loss'].append(val_kl_loss)\n",
    "\n",
    "    pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
    "    # evaluate on samples\n",
    "    sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
    "    sample_history[epoch_idx] = sample_res\n",
    "\n",
    "    # save model weights\n",
    "    torch.save({\n",
    "                'epoch': epoch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss\n",
    "                }, model_root/f'model_{epoch_idx:03d}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ix9M7rs8bbvE",
    "outputId": "9f86e18d-de2d-480e-bae5-33f8f5f84ce2"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "TBFPvoh3bVbM",
    "outputId": "1b7b6827-7a3d-4176-8b54-4ab01b5de29e"
   },
   "outputs": [],
   "source": [
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 981
    },
    "id": "invB2CcVtAem",
    "outputId": "92645aa4-2ea7-497b-c739-83e0f868afab"
   },
   "outputs": [],
   "source": [
    "plot_samples(sample_history, samples=samples, epoch_stride=5, fig_scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6ltegFux_Kg"
   },
   "source": [
    "Visualize reconstruciton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRj0w-IdbVbP"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "single_el_idx = samples['single_el_idx']\n",
    "images_noisy = samples['images_noisy'][single_el_idx, 0]\n",
    "images = samples['images'][single_el_idx, 0]\n",
    "\n",
    "smpl_ims = []\n",
    "for epoch_idx, hist_el in sample_history.items():\n",
    "    samples_arr = [images_noisy, hist_el['y'][single_el_idx, 0], images]\n",
    "    smpl_ims.append(samples_arr)\n",
    "\n",
    "ny, nx = len(smpl_ims[0]), len(smpl_ims[0][0])\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above animations use JavaScript\n",
    "\n",
    "s=1\n",
    "fig = plt.figure(figsize=(s*nx, s*ny))\n",
    "\n",
    "m = mosaic(smpl_ims[0])\n",
    "\n",
    "ttl = plt.title(f'after epoch {int(0)}')\n",
    "# plot 0th epoch - 0th frame\n",
    "imsh = plt.imshow(m, cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "# this function will be called to render each of the frames\n",
    "def animate(i):\n",
    "    m = mosaic(smpl_ims[i])\n",
    "    imsh.set_data(m)\n",
    "\n",
    "    ttl.set_text(f'after epoch {i}')\n",
    "\n",
    "    return imsh\n",
    "\n",
    "# create animation\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(smpl_ims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "Mj6X_wLbVeae",
    "outputId": "41e7a429-2525-4110-a75f-08cacfe84c20"
   },
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPlUHfTWXbET"
   },
   "source": [
    "Let's look at the latent representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rdrCY9ftAe0",
    "outputId": "b6df3e52-fcea-4680-985e-00c4786f02d5"
   },
   "outputs": [],
   "source": [
    "sample_history[0]['z'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UrvukYbxXciL"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above, uses JavaScript\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "smpl_epochs = list(sample_history.keys())\n",
    "\n",
    "z_res_mean = [sample_history[ep]['z'][0] for ep in smpl_epochs]\n",
    "z_logvar = [sample_history[ep]['z'][1] for ep in smpl_epochs]\n",
    "z_res_std = np.exp(np.array(z_logvar) * .5)\n",
    "\n",
    "\n",
    "l_res = samples['labels']\n",
    "l_col = [cm.rainbow(l_i/10) for l_i in l_res]\n",
    "\n",
    "# error bars:\n",
    "def get_lines(m, s):\n",
    "  l = []\n",
    "  for (x, y), (sx, sy) in zip(m, s):\n",
    "    l.append([(x-sx, y), (x+sx, y)]) # h\n",
    "    l.append([(x, y-sy), (x, y+sy)]) # w\n",
    "  return np.array(l)\n",
    "\n",
    "lines = get_lines(z_res_mean[0], z_res_std[0])\n",
    "lc = mc.LineCollection(lines, color=l_col, linewidths=2, alpha=0.3)\n",
    "fig.gca().add_collection(lc)\n",
    "\n",
    "scat = plt.scatter(z_res_mean[0][:,0], z_res_mean[0][:,1], c=l_res, cmap=cm.rainbow)\n",
    "#scat_err = plt.errorbar(z_res_mean[0][:,0], z_res_mean[0][:,1], xerr=z_res_std[0][:,0], yerr=z_res_std[0][:,1], fmt=\"o\")\n",
    "\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "\n",
    "legend1 = plt.gca().legend(*scat.legend_elements(), title=\"digits\")\n",
    "plt.gca().add_artist(legend1)\n",
    "plt.gca().set_aspect('equal')\n",
    "ttl = plt.title(f'after epoch {0}')\n",
    "\n",
    "def animate(i):\n",
    "    z = z_res_mean[i]\n",
    "    scat.set_offsets(z)\n",
    "\n",
    "    lines = get_lines(z_res_mean[i], z_res_std[i])\n",
    "    lc.set_segments(lines)\n",
    "\n",
    "    ttl.set_text(f'after epoch {i}')\n",
    "    return scat, lc\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(z_res_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 898
    },
    "id": "XOoFVobIVXVD",
    "outputId": "570619c9-2a41-4885-f826-df18c1df9fb7"
   },
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmCFt4HVzcQs"
   },
   "source": [
    "And let's sample from the latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87T1eLW9tAe2",
    "outputId": "cf570f0a-e9d3-4fb0-d7d4-71eb0b5e6473"
   },
   "outputs": [],
   "source": [
    "zs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5m4ArVRRXJI_",
    "outputId": "b85ad690-b64f-478b-d772-89efaf57ef1b"
   },
   "outputs": [],
   "source": [
    "n = 20\n",
    "zs = np.meshgrid(norm.ppf(np.linspace(0.0001, 0.9999, n)),  # sampling proportionally to the density\n",
    "                 norm.ppf(np.linspace(0.0001, 0.9999, n)))\n",
    "# zs = np.meshgrid(np.linspace(-4, 4, n),  # sampling linearly\n",
    "#                  np.linspace(-4, 4, n))\n",
    "zs = np.stack(zs, axis=-1)\n",
    "zs = zs.reshape((-1, 2))\n",
    "\n",
    "zs_t = torch.tensor(zs, dtype=torch.float32).to(device)\n",
    "\n",
    "def fn(ae):\n",
    "  with torch.no_grad():\n",
    "    ims_t = ae.decode(zs_t)\n",
    "    ims = ims_t.detach().cpu().numpy()\n",
    "    ims = ims[:, 0, :, :]  # remove channel dimension\n",
    "\n",
    "  sh = list(ims.shape)\n",
    "  ims = ims.reshape([n, n]+sh[1:])\n",
    "  plt.figure(figsize=(n, n))\n",
    "  plt.imshow(mosaic(ims[::-1]), vmin=0, vmax=1, cmap='gray')\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "run_on_trained(model, model_root, fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vnLTJeN1fYz6",
    "outputId": "437fc3c5-1f0d-4870-f2a7-5a8f3c95c5aa"
   },
   "outputs": [],
   "source": [
    "def fn(ae):\n",
    "  # x = samples['images_noisy']\n",
    "  # l = samples['labels']\n",
    "  # ds = valid_dataset\n",
    "  ds = train_dataset\n",
    "  x = ds.data.numpy().reshape(-1, 1, 28, 28)\n",
    "  x = x/255.*2-1\n",
    "  l = ds.targets.numpy()\n",
    "  x_t = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "  z_m, z_s = ae.encode(x_t)\n",
    "  z_m, z_s = [el.detach().cpu().numpy() for el in [z_m, z_s]]\n",
    "\n",
    "  z_res_std = np.exp(np.array(z_s) * .5)\n",
    "\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  scat = plt.scatter(*z_m.T, c=l, s=2, cmap='jet')\n",
    "  plt.gca().legend(*scat.legend_elements(), title=\"digits\")\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(10, 2), ncols=2, nrows=1)\n",
    "  ax[0].hist(z_m.flatten(), bins=100);\n",
    "  ax[1].hist(z_res_std.flatten(), bins=100);\n",
    "  ax[0].set_title('mean')\n",
    "  ax[1].set_title('std')\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "run_on_trained(model, model_root, fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PLAxq3XIho_"
   },
   "source": [
    "## Visualization excercise (homework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_81bI_-MYTo3"
   },
   "source": [
    "Option 1: Sample 31x31 and overlay with data points.\n",
    "\n",
    "Option 2: Visulize data-point in latent space\n",
    "\n",
    "\n",
    "```\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.gca()\n",
    "\n",
    "# given the image sample array `image_arr` and the pair of coordinate arrays\n",
    "# of the latent representation z0_arr, z1_arr:\n",
    "for z0_i, z1_i, im_i in zip(z0_arr, z1_arr, image_arr):\n",
    "    im = OffsetImage(im_i, zoom=0.5)\n",
    "    ab = AnnotationBbox(im, (z0_i, z1_i), xycoords='data', frameon=False)\n",
    "\n",
    "    ax.add_artist(ab)\n",
    "    ax.update_datalim([(z0_i, z1_i)])\n",
    "    ax.autoscale()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3DrSyps5wWv"
   },
   "source": [
    "## Example of project work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "642wgmpp50IC"
   },
   "source": [
    "Use VAE to explore Fashion MNIST, or any other dataset\n",
    " 1. train model\n",
    " 2. explore latent representation\n",
    " 3. find outliers\n",
    " 4. sample from the latent distribution\n",
    " 5. overlay with data points\n",
    " 6. Train classifier/regressor using the latent space representation of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGdgxx9LngGC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "smjlo1x48N3t",
    "TVG7kpjf8T2D",
    "cbR9emfD8d41",
    "9QH3J3Z-jTb2",
    "7-YdzZ2QDwLg",
    "4saA2FcDFcMq",
    "tu5h3HaT6aCN",
    "AVUXN5Jq7kx9",
    "9qArUdx6YHzD",
    "Tk1sA3cNFuLB",
    "phTJlLGx3a5H",
    "8_tWAYpah3aG",
    "9PLAxq3XIho_",
    "lgVvSoXa5vOk",
    "G3DrSyps5wWv",
    "kHPnzzd_6PLO",
    "YVruUa0d7fMw",
    "G1_R6CnAy7-F"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00e2ab4b7e60439aa1bd61bdf4781bb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f5f98a52e824aeb921b99cb6f3527bc",
       "IPY_MODEL_c4f4e17e87904108b4442cdb36443429",
       "IPY_MODEL_f3e816556c95470794f40759a8d8c58e"
      ],
      "layout": "IPY_MODEL_508a0fc496f445dbb37a177c3b2ff4bf"
     }
    },
    "03b115e9fd9e4f6bacee75bf1e91735f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05eed87241de4972a7acc70f48c3430c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0d97d53e4453459ab1ed6f7e49c1d6d6",
       "IPY_MODEL_7e7868cebe47479e82ea30763c86edca",
       "IPY_MODEL_d607db3bc256454092cb1eb024901c20"
      ],
      "layout": "IPY_MODEL_f03adde2b61040daa05a9a981512af51"
     }
    },
    "0676ba35b6194a078f53b30ad05f4991": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d97d53e4453459ab1ed6f7e49c1d6d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb197b7ef6d6484e80a4f77f6f03b1e7",
      "placeholder": "​",
      "style": "IPY_MODEL_8bc1b3f02e3543989bacf0aa882589e0",
      "value": "Generating valid split: "
     }
    },
    "0da5b7e0ce25441cba444e632ef62340": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0fd05eefe82d4e63b9fda9a6124e5153": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fedb0e6df744ad7b32dd5f14a493b61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1179f40122794b168dc0665d77b3bd15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1623c03a91fa4f80b9109e281bda1a32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b321467d620a4a908cd906a6c6d28a17",
       "IPY_MODEL_fae564eac5eb4d95a5fda6b3e6b083de",
       "IPY_MODEL_c705c3f1b21e4f41a75fbe5319e48383"
      ],
      "layout": "IPY_MODEL_48d161e77c1d4e40beecb0f263d54763"
     }
    },
    "1abbc6518ef7445bbff2559b1a7cca2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32b5a64c38474eb3b05eb1cde1994927": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3494b43c49c14b009a7e03ae6798f818": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "35d9922f836f4fd2b0cdfe447af09bca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3867dfbe772141b58ff8728a201f76dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81d4444cb8414b56990835b2ed99677e",
      "placeholder": "​",
      "style": "IPY_MODEL_7802e2b380da47a2979a6e4ec29da921",
      "value": " 60000/0 [00:00&lt;00:00, 83765.10 examples/s]"
     }
    },
    "3944d43006094c3ead030b6ffe505815": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3cb558d03c524075abfc67a154ec4f0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3494b43c49c14b009a7e03ae6798f818",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5817af58e0c443f5bd226645b0157e78",
      "value": 1
     }
    },
    "474e9082a92c45d295fcc8faa09fe4af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5eda53205b14617b72c40919dc33d65",
      "placeholder": "​",
      "style": "IPY_MODEL_d759c3f33a6342ca838aa7935997ceea",
      "value": "100%"
     }
    },
    "48d161e77c1d4e40beecb0f263d54763": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b48e00446464125ad5f139ba9e68c7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e04bb3c232a469bac1001aa5ac8a561": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a537b8225fb743db9c3054959b2c2234",
      "placeholder": "​",
      "style": "IPY_MODEL_d8dfe23f85134fd6b02e07e2fc2aa25d",
      "value": "100%"
     }
    },
    "508a0fc496f445dbb37a177c3b2ff4bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55f4b904d9404e8cbf9e1a5c8c934436": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5817af58e0c443f5bd226645b0157e78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5980b89f1166499ba6185311002463f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fd05eefe82d4e63b9fda9a6124e5153",
      "placeholder": "​",
      "style": "IPY_MODEL_0676ba35b6194a078f53b30ad05f4991",
      "value": " 20/20 [05:48&lt;00:00, 17.36s/it, epoch=20/20, loss=0.0452, val_loss=0.0452]"
     }
    },
    "5ec243527f6540009166f1147122d4f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "638d985fe9da4d2aa7d34447049be1f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6e0683251574417793ac741546fcafef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7802e2b380da47a2979a6e4ec29da921": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d6cf4cf803745e4b959b89eb3244256": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d7ef7296f0a41049bf1c81a65987a4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4ea9a4bb14e4df19d105c81ce6f2e6d",
      "placeholder": "​",
      "style": "IPY_MODEL_80dabca945a246a288c9556aeb303da4",
      "value": " 50/50 [11:45&lt;00:00, 14.07s/it, epoch=50/50, loss=0.0301, val_loss=0.0300]"
     }
    },
    "7e7868cebe47479e82ea30763c86edca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec4522af12ec4cdf80d3c29d425ac273",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0da5b7e0ce25441cba444e632ef62340",
      "value": 1
     }
    },
    "7ed5c6c6f5c749b2849b203da467f8e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ee5ddf3b47b46feaa3eeb372f489c98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "80dabca945a246a288c9556aeb303da4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "81d4444cb8414b56990835b2ed99677e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8552e02fdd1b480bb09e64af54fb2829": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03b115e9fd9e4f6bacee75bf1e91735f",
      "placeholder": "​",
      "style": "IPY_MODEL_3944d43006094c3ead030b6ffe505815",
      "value": "Generating train split: "
     }
    },
    "85c0efd7ac52436a83aeac87dc34f960": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8bc1b3f02e3543989bacf0aa882589e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f47b7e2b591402eba7cafdfd404a01c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8f5f98a52e824aeb921b99cb6f3527bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d6cf4cf803745e4b959b89eb3244256",
      "placeholder": "​",
      "style": "IPY_MODEL_deeec5762ea54fbcb136e47696434be9",
      "value": "100%"
     }
    },
    "8f9a49a562f341bbbb71a27a06a6ad1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4e04bb3c232a469bac1001aa5ac8a561",
       "IPY_MODEL_db4cb3d76be84a52830ddb970ab3fe89",
       "IPY_MODEL_5980b89f1166499ba6185311002463f2"
      ],
      "layout": "IPY_MODEL_1179f40122794b168dc0665d77b3bd15"
     }
    },
    "94baa119ed9845928991ca3440029131": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94db787b4fe04f3a9aafccabc73e9f31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "983dbd1084cd43c1835a398d9a51d22e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_474e9082a92c45d295fcc8faa09fe4af",
       "IPY_MODEL_f793438494ab4d27acb3c57705c7bffd",
       "IPY_MODEL_7d7ef7296f0a41049bf1c81a65987a4c"
      ],
      "layout": "IPY_MODEL_32b5a64c38474eb3b05eb1cde1994927"
     }
    },
    "9936fe021fa74ea2a3429cacbfb252cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c09c1eba258404f870ba0895b638570": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a537b8225fb743db9c3054959b2c2234": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b321467d620a4a908cd906a6c6d28a17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55f4b904d9404e8cbf9e1a5c8c934436",
      "placeholder": "​",
      "style": "IPY_MODEL_7ee5ddf3b47b46feaa3eeb372f489c98",
      "value": "100%"
     }
    },
    "b58249e4ec5a467fb40c70917b0f2eaa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8552e02fdd1b480bb09e64af54fb2829",
       "IPY_MODEL_3cb558d03c524075abfc67a154ec4f0d",
       "IPY_MODEL_3867dfbe772141b58ff8728a201f76dc"
      ],
      "layout": "IPY_MODEL_7ed5c6c6f5c749b2849b203da467f8e2"
     }
    },
    "b5eda53205b14617b72c40919dc33d65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4ea9a4bb14e4df19d105c81ce6f2e6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4f4e17e87904108b4442cdb36443429": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35d9922f836f4fd2b0cdfe447af09bca",
      "max": 15,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_638d985fe9da4d2aa7d34447049be1f6",
      "value": 15
     }
    },
    "c705c3f1b21e4f41a75fbe5319e48383": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fedb0e6df744ad7b32dd5f14a493b61",
      "placeholder": "​",
      "style": "IPY_MODEL_9c09c1eba258404f870ba0895b638570",
      "value": " 70/70 [21:09&lt;00:00, 18.25s/it, epoch=70/70, loss=0.0843, val_loss=0.0841]"
     }
    },
    "cb8f00dac91e47b3b81084de7ac52e62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d607db3bc256454092cb1eb024901c20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94db787b4fe04f3a9aafccabc73e9f31",
      "placeholder": "​",
      "style": "IPY_MODEL_9936fe021fa74ea2a3429cacbfb252cb",
      "value": " 10000/0 [00:00&lt;00:00, 91384.75 examples/s]"
     }
    },
    "d759c3f33a6342ca838aa7935997ceea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8dfe23f85134fd6b02e07e2fc2aa25d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db4cb3d76be84a52830ddb970ab3fe89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e0683251574417793ac741546fcafef",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5ec243527f6540009166f1147122d4f1",
      "value": 20
     }
    },
    "deeec5762ea54fbcb136e47696434be9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec4522af12ec4cdf80d3c29d425ac273": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "f03adde2b61040daa05a9a981512af51": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3e816556c95470794f40759a8d8c58e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94baa119ed9845928991ca3440029131",
      "placeholder": "​",
      "style": "IPY_MODEL_85c0efd7ac52436a83aeac87dc34f960",
      "value": " 15/15 [03:57&lt;00:00, 15.77s/it, epoch=15/15, loss=0.0342, val_loss=0.0339]"
     }
    },
    "f793438494ab4d27acb3c57705c7bffd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1abbc6518ef7445bbff2559b1a7cca2f",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb8f00dac91e47b3b81084de7ac52e62",
      "value": 50
     }
    },
    "fae564eac5eb4d95a5fda6b3e6b083de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b48e00446464125ad5f139ba9e68c7b",
      "max": 70,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8f47b7e2b591402eba7cafdfd404a01c",
      "value": 70
     }
    },
    "fb197b7ef6d6484e80a4f77f6f03b1e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
